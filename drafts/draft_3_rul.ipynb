{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "column_names = ['engine_id', 'cycle', 'setting1', 'setting2', 'setting3'] + [f'sensor{i}' for i in range(1, 27)]\n",
    "train_df = pd.read_csv('./dataset/train_FD001.txt', sep='\\s+', header=None, names=column_names)\n",
    "test_df = pd.read_csv('./dataset/test_FD001.txt', sep='\\s+', header=None, names=column_names)\n",
    "true_rul = pd.read_csv('./dataset/RUL_FD001.txt', header=None)\n",
    "train_df = train_df.dropna(axis=1, how=\"all\")\n",
    "test_df = test_df.dropna(axis=1, how=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNG\n",
    "rng = np.random.RandomState(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = [\"setting3\", \"sensor1\", \"sensor5\", \"sensor6\", \"sensor10\", \"sensor16\", \"sensor18\", \"sensor19\"]\n",
    "train_df_dropped = train_df.drop(columns=columns_to_drop)\n",
    "test_df_dropped = test_df.drop(columns=columns_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized Data (0-1 range):\n",
      "   engine_id  cycle  setting1  setting2   sensor2   sensor3   sensor4  \\\n",
      "0          1      1  0.459770  0.166667  0.183735  0.406802  0.309757   \n",
      "1          1      2  0.609195  0.250000  0.283133  0.453019  0.352633   \n",
      "2          1      3  0.252874  0.750000  0.343373  0.369523  0.370527   \n",
      "3          1      4  0.540230  0.500000  0.343373  0.256159  0.331195   \n",
      "4          1      5  0.390805  0.333333  0.349398  0.257467  0.404625   \n",
      "\n",
      "    sensor7   sensor8   sensor9  sensor11  sensor12  sensor13  sensor14  \\\n",
      "0  0.726248  0.242424  0.109755  0.369048  0.633262  0.205882  0.199608   \n",
      "1  0.628019  0.212121  0.100242  0.380952  0.765458  0.279412  0.162813   \n",
      "2  0.710145  0.272727  0.140043  0.250000  0.795309  0.220588  0.171793   \n",
      "3  0.740741  0.318182  0.124518  0.166667  0.889126  0.294118  0.174889   \n",
      "4  0.668277  0.242424  0.149960  0.255952  0.746269  0.235294  0.174734   \n",
      "\n",
      "   sensor15  sensor17  sensor20  sensor21  \n",
      "0  0.363986  0.333333  0.713178  0.724662  \n",
      "1  0.411312  0.333333  0.666667  0.731014  \n",
      "2  0.357445  0.166667  0.627907  0.621375  \n",
      "3  0.166603  0.333333  0.573643  0.662386  \n",
      "4  0.402078  0.416667  0.589147  0.704502  \n"
     ]
    }
   ],
   "source": [
    "# Normalization\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Initialize the scaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Separate the columns to normalize and the columns to skip\n",
    "columns_to_skip = train_df_dropped.columns[:2]\n",
    "columns_to_normalize = train_df_dropped.columns[2:]\n",
    "\n",
    "# Normalize only the selected columns\n",
    "normalized_data = scaler.fit_transform(train_df_dropped[columns_to_normalize])\n",
    "\n",
    "# Combine the normalized and unnormalized columns\n",
    "train_df_normalized = pd.DataFrame(train_df_dropped[columns_to_skip].values, columns=columns_to_skip)\n",
    "train_df_normalized = pd.concat([train_df_normalized, pd.DataFrame(normalized_data, columns=columns_to_normalize)], axis=1)\n",
    "\n",
    "# Display the normalized DataFrame\n",
    "print(\"Normalized Data (0-1 range):\")\n",
    "print(train_df_normalized.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply column dropping to test data\n",
    "test_df_dropped = test_df.drop(columns=columns_to_drop)\n",
    "\n",
    "# Normalize test data using the same scaler\n",
    "normalized_test_data = scaler.transform(test_df_dropped[columns_to_normalize])\n",
    "\n",
    "# Combine normalized and unnormalized columns in test data\n",
    "test_df_normalized = pd.DataFrame(test_df_dropped[columns_to_skip].values, columns=columns_to_skip)\n",
    "test_df_normalized = pd.concat([test_df_normalized, pd.DataFrame(normalized_test_data, columns=columns_to_normalize)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labelling RUL\n",
    "train_df_normalized['RUL'] = train_df_normalized.groupby('engine_id')['cycle'].transform(lambda x: x.max() - x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PWRUL\n",
    "# Set the early RUL threshold\n",
    "early_rul_threshold = 120\n",
    "\n",
    "# Define the piecewise linear degradation function\n",
    "def piecewise_rul(cycle, max_cycle):\n",
    "    remaining_life = max_cycle - cycle\n",
    "    if remaining_life > early_rul_threshold:\n",
    "        return early_rul_threshold  # slower degradation in the early phase\n",
    "    else:\n",
    "        return remaining_life  # direct linear degradation after threshold\n",
    "    \n",
    "train_df_normalized[\"PWRUL\"] = train_df_normalized.apply(lambda row: piecewise_rul(row['cycle'], row['cycle'] + row['RUL']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: (17631, 30, 16)\n",
      "Labels shape: (17631,)\n"
     ]
    }
   ],
   "source": [
    "# Define sequence length\n",
    "sequence_length = 30\n",
    "\n",
    "# Identify feature columns\n",
    "feature_columns = [col for col in train_df_normalized.columns if col not in ['engine_id', 'cycle', 'RUL', 'PWRUL']]\n",
    "\n",
    "# Initialize lists for sequences and labels\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "# Generate sequences and labels\n",
    "for engine_id in train_df_normalized['engine_id'].unique():\n",
    "    engine_data = train_df_normalized[train_df_normalized['engine_id'] == engine_id].reset_index(drop=True)\n",
    "    for i in range(sequence_length, len(engine_data)):\n",
    "        # Extract sequence of sensor readings\n",
    "        seq_x = engine_data[feature_columns].iloc[i-sequence_length:i].values\n",
    "        # Extract the RUL value at the end of the sequence\n",
    "        seq_y = engine_data['RUL'].iloc[i]\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "\n",
    "# Convert to NumPy arrays\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "print(\"Input shape:\", X.shape)\n",
    "print(\"Labels shape:\", y.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1d (Conv1D)             (None, 28, 64)            3136      \n",
      "                                                                 \n",
      " conv1d_1 (Conv1D)           (None, 26, 64)            12352     \n",
      "                                                                 \n",
      " max_pooling1d (MaxPooling1D  (None, 13, 64)           0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 100)               66000     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 101       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 81,589\n",
      "Trainable params: 81,589\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, LSTM, Dense\n",
    "\n",
    "# Define the model\n",
    "model = Sequential()\n",
    "model.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(sequence_length, len(feature_columns))))\n",
    "model.add(Conv1D(filters=64, kernel_size=3, activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(LSTM(units=100))\n",
    "model.add(Dense(units=1))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "276/276 [==============================] - 4s 9ms/step - loss: 8626.5986\n",
      "Epoch 2/50\n",
      "276/276 [==============================] - 3s 9ms/step - loss: 5810.3540\n",
      "Epoch 3/50\n",
      "276/276 [==============================] - 3s 9ms/step - loss: 4531.9673\n",
      "Epoch 4/50\n",
      "276/276 [==============================] - 3s 9ms/step - loss: 4017.7747\n",
      "Epoch 5/50\n",
      "276/276 [==============================] - 3s 9ms/step - loss: 3853.6587\n",
      "Epoch 6/50\n",
      "276/276 [==============================] - 3s 9ms/step - loss: 3783.4714\n",
      "Epoch 7/50\n",
      "276/276 [==============================] - 3s 9ms/step - loss: 2209.8823\n",
      "Epoch 8/50\n",
      "276/276 [==============================] - 3s 9ms/step - loss: 1691.7799\n",
      "Epoch 9/50\n",
      "276/276 [==============================] - 3s 10ms/step - loss: 1432.3540\n",
      "Epoch 10/50\n",
      "276/276 [==============================] - 3s 10ms/step - loss: 1235.6654\n",
      "Epoch 11/50\n",
      "276/276 [==============================] - 3s 10ms/step - loss: 1146.3425\n",
      "Epoch 12/50\n",
      "276/276 [==============================] - 3s 10ms/step - loss: 1068.2013\n",
      "Epoch 13/50\n",
      "276/276 [==============================] - 3s 10ms/step - loss: 1024.7351\n",
      "Epoch 14/50\n",
      "276/276 [==============================] - 3s 10ms/step - loss: 981.8453\n",
      "Epoch 15/50\n",
      "276/276 [==============================] - 3s 10ms/step - loss: 950.2339\n",
      "Epoch 16/50\n",
      "276/276 [==============================] - 3s 10ms/step - loss: 907.0503\n",
      "Epoch 17/50\n",
      "276/276 [==============================] - 3s 10ms/step - loss: 889.6989\n",
      "Epoch 18/50\n",
      "276/276 [==============================] - 3s 10ms/step - loss: 852.1208\n",
      "Epoch 19/50\n",
      "276/276 [==============================] - 3s 10ms/step - loss: 852.0795\n",
      "Epoch 20/50\n",
      "276/276 [==============================] - 3s 10ms/step - loss: 828.9869\n",
      "Epoch 21/50\n",
      "276/276 [==============================] - 3s 10ms/step - loss: 807.7731\n",
      "Epoch 22/50\n",
      "276/276 [==============================] - 3s 10ms/step - loss: 773.8365\n",
      "Epoch 23/50\n",
      "276/276 [==============================] - 3s 10ms/step - loss: 777.2534\n",
      "Epoch 24/50\n",
      "276/276 [==============================] - 3s 10ms/step - loss: 759.1450\n",
      "Epoch 25/50\n",
      "276/276 [==============================] - 3s 10ms/step - loss: 738.4724\n",
      "Epoch 26/50\n",
      "276/276 [==============================] - 3s 10ms/step - loss: 725.5428\n",
      "Epoch 27/50\n",
      "276/276 [==============================] - 3s 10ms/step - loss: 722.5394\n",
      "Epoch 28/50\n",
      "276/276 [==============================] - 3s 10ms/step - loss: 693.9079\n",
      "Epoch 29/50\n",
      "276/276 [==============================] - 3s 10ms/step - loss: 687.8206\n",
      "Epoch 30/50\n",
      "276/276 [==============================] - 3s 10ms/step - loss: 689.7881\n",
      "Epoch 31/50\n",
      "276/276 [==============================] - 3s 10ms/step - loss: 661.0604\n",
      "Epoch 32/50\n",
      "276/276 [==============================] - 3s 11ms/step - loss: 663.3780\n",
      "Epoch 33/50\n",
      "276/276 [==============================] - 3s 11ms/step - loss: 646.1006\n",
      "Epoch 34/50\n",
      "276/276 [==============================] - 3s 10ms/step - loss: 608.9761\n",
      "Epoch 35/50\n",
      "276/276 [==============================] - 3s 10ms/step - loss: 604.3461\n",
      "Epoch 36/50\n",
      "276/276 [==============================] - 3s 10ms/step - loss: 613.7045\n",
      "Epoch 37/50\n",
      "276/276 [==============================] - 3s 10ms/step - loss: 607.6440\n",
      "Epoch 38/50\n",
      "276/276 [==============================] - 3s 10ms/step - loss: 569.5879\n",
      "Epoch 39/50\n",
      "276/276 [==============================] - 3s 10ms/step - loss: 555.5875\n",
      "Epoch 40/50\n",
      "276/276 [==============================] - 3s 10ms/step - loss: 518.1508\n",
      "Epoch 41/50\n",
      "276/276 [==============================] - 3s 10ms/step - loss: 517.7499\n",
      "Epoch 42/50\n",
      "276/276 [==============================] - 3s 10ms/step - loss: 531.9296\n",
      "Epoch 43/50\n",
      "276/276 [==============================] - 3s 10ms/step - loss: 495.6923\n",
      "Epoch 44/50\n",
      "276/276 [==============================] - 3s 10ms/step - loss: 506.0955\n",
      "Epoch 45/50\n",
      "276/276 [==============================] - 3s 10ms/step - loss: 485.5662\n",
      "Epoch 46/50\n",
      "276/276 [==============================] - 3s 10ms/step - loss: 495.1196\n",
      "Epoch 47/50\n",
      "276/276 [==============================] - 3s 10ms/step - loss: 455.6490\n",
      "Epoch 48/50\n",
      "276/276 [==============================] - 3s 10ms/step - loss: 460.9640\n",
      "Epoch 49/50\n",
      "276/276 [==============================] - 3s 10ms/step - loss: 441.5601\n",
      "Epoch 50/50\n",
      "276/276 [==============================] - 3s 10ms/step - loss: 407.7502\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X, y, \n",
    "                    epochs=50, \n",
    "                    batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# # Predict RUL on validation data\n",
    "# y_pred = model.predict(X_val)\n",
    "\n",
    "# # Calculate Mean Squared Error\n",
    "# mse = mean_squared_error(y_val, y_pred)\n",
    "# print(\"Validation Mean Squared Error:\", mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 3ms/step\n",
      "Test Mean Squared Error: 984.0605567679921\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "X_test = []\n",
    "y_test = []\n",
    "\n",
    "for engine_id in test_df_normalized['engine_id'].unique():\n",
    "    engine_data = test_df_normalized[test_df_normalized['engine_id'] == engine_id].reset_index(drop=True)\n",
    "    if len(engine_data) >= sequence_length:\n",
    "        # Use only the last sequence\n",
    "        seq_x = engine_data[feature_columns].iloc[-sequence_length:].values\n",
    "        X_test.append(seq_x)\n",
    "        # Get the true RUL for this engine\n",
    "        seq_y = true_rul.loc[engine_id - 1].values[0]\n",
    "        y_test.append(seq_y)\n",
    "    else:\n",
    "        print(f\"Engine {engine_id} has insufficient data for the defined sequence length.\")\n",
    "\n",
    "X_test = np.array(X_test)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "# Predict RUL on test data\n",
    "y_test_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "test_mse = mean_squared_error(y_test, y_test_pred)\n",
    "print(\"Test Mean Squared Error:\", test_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Root Mean Squared Error: 31.3697395074934\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yshen\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "rmse = mean_squared_error(y_test, y_test_pred, squared=False)\n",
    "print(\"Test Root Mean Squared Error:\", rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save('rul_prediction_model.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
