{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = ['engine_id', 'cycle', 'setting1', 'setting2', 'setting3'] + [f'sensor{i}' for i in range(1, 22)] # Last 5 columns all NA\n",
    "train_df = pd.read_csv('./dataset/train_FD001.txt', sep='\\s+', header=None, names=column_names)\n",
    "test_df = pd.read_csv('./dataset/test_FD001.txt', sep='\\s+', header=None, names=column_names)\n",
    "true_rul = pd.read_csv('./dataset/RUL_FD001.txt', header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KIV: Should we include settings?\n",
    "# Dropping columns\n",
    "columns_to_drop = [\"setting1\", \"setting2\", \"setting3\", \"sensor1\", \"sensor5\", \"sensor6\", \"sensor10\", \"sensor16\", \"sensor18\", \"sensor19\"]\n",
    "train_df_dropped = train_df.drop(columns=columns_to_drop)\n",
    "test_df_dropped = test_df.drop(columns=columns_to_drop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Labelling Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labelling RUL\n",
    "# train_df_dropped['RUL'] = train_df_dropped.groupby('engine_id')['cycle'].transform(lambda x: x.max() - x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature extraction for TSFRESH\n",
    "from tsfresh import extract_features\n",
    "from tsfresh.feature_extraction import ComprehensiveFCParameters\n",
    " \n",
    "# Combine your 30-cycle windowed series with unit IDs as a DataFrame for tsfresh\n",
    "train_series_df = pd.DataFrame(train_df_dropped)\n",
    "train_series_df[\"engine_id\"] = train_df_dropped[\"engine_id\"]\n",
    "# Ensure each unit has a 'time' column cycling from 0 to 29\n",
    "train_series_df[\"time\"] = train_series_df.groupby(\"engine_id\").cumcount() % 30\n",
    " \n",
    "# Set up tsfresh parameters for comprehensive feature extraction\n",
    "settings = ComprehensiveFCParameters()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feature Extraction:   0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feature Extraction: 100%|██████████| 20/20 [00:39<00:00,  1.98s/it]\n"
     ]
    }
   ],
   "source": [
    "# Extract features using tsfresh\n",
    "train_features = extract_features(\n",
    "    train_series_df,\n",
    "    column_id=\"engine_id\",\n",
    "    column_sort=\"time\",\n",
    "    default_fc_parameters=settings,\n",
    "    n_jobs=4,  # Parallelize if possible to speed up extraction\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 11745)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaling of Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized Data (0-1 range):\n",
      "   engine_id  cycle   sensor2   sensor3   sensor4   sensor7   sensor8  \\\n",
      "0          1      1  0.183735  0.406802  0.309757  0.726248  0.242424   \n",
      "1          1      2  0.283133  0.453019  0.352633  0.628019  0.212121   \n",
      "2          1      3  0.343373  0.369523  0.370527  0.710145  0.272727   \n",
      "3          1      4  0.343373  0.256159  0.331195  0.740741  0.318182   \n",
      "4          1      5  0.349398  0.257467  0.404625  0.668277  0.242424   \n",
      "\n",
      "    sensor9  sensor11  sensor12  sensor13  sensor14  sensor15  sensor17  \\\n",
      "0  0.109755  0.369048  0.633262  0.205882  0.199608  0.363986  0.333333   \n",
      "1  0.100242  0.380952  0.765458  0.279412  0.162813  0.411312  0.333333   \n",
      "2  0.140043  0.250000  0.795309  0.220588  0.171793  0.357445  0.166667   \n",
      "3  0.124518  0.166667  0.889126  0.294118  0.174889  0.166603  0.333333   \n",
      "4  0.149960  0.255952  0.746269  0.235294  0.174734  0.402078  0.416667   \n",
      "\n",
      "   sensor20  sensor21  \n",
      "0  0.713178  0.724662  \n",
      "1  0.666667  0.731014  \n",
      "2  0.627907  0.621375  \n",
      "3  0.573643  0.662386  \n",
      "4  0.589147  0.704502  \n"
     ]
    }
   ],
   "source": [
    "# Normalization\n",
    "# KIV: Standardization\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Initialize the scaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Separate the columns to normalize and the columns to skip\n",
    "columns_to_skip = train_df_dropped.columns[:2]\n",
    "columns_to_normalize = train_df_dropped.columns[2:]\n",
    "\n",
    "# Normalize only the selected columns\n",
    "normalized_data = scaler.fit_transform(train_df_dropped[columns_to_normalize])\n",
    "\n",
    "# Combine the normalized and unnormalized columns\n",
    "train_df_normalized = pd.DataFrame(train_df_dropped[columns_to_skip].values, columns=columns_to_skip)\n",
    "train_df_normalized = pd.concat([train_df_normalized, pd.DataFrame(normalized_data, columns=columns_to_normalize)], axis=1)\n",
    "\n",
    "# Display the normalized DataFrame\n",
    "print(\"Normalized Data (0-1 range):\")\n",
    "print(train_df_normalized.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train, Validate, Test approach\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Labelling\n",
    "# RUL Column\n",
    "# Label Column\n",
    "# Setting of early_rul"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
