{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "rng = np.random.RandomState(42)\n",
    "column_names = ['engine_id', 'cycle', 'setting1', 'setting2', 'setting3'] + [f'sensor{i}' for i in range(1, 27)]\n",
    "train_df = pd.read_csv('./dataset/train_FD001.txt', sep='\\s+', header=None, names=column_names)\n",
    "test_df = pd.read_csv('./dataset/test_FD001.txt', sep='\\s+', header=None, names=column_names)\n",
    "true_rul = pd.read_csv('./dataset/RUL_FD001.txt', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature extraction with TSFRESH\n",
    "from tsfresh import extract_features, select_features\n",
    "from tsfresh.utilities.dataframe_functions import impute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.dropna(axis=1, how=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = [\"setting3\", \"sensor1\", \"sensor5\", \"sensor6\", \"sensor10\", \"sensor16\", \"sensor18\", \"sensor19\"]\n",
    "train_df_dropped = train_df.drop(columns=columns_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Initialize the scaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Separate the columns to normalize and the columns to skip\n",
    "columns_to_skip = train_df_dropped.columns[:2]\n",
    "columns_to_normalize = train_df_dropped.columns[2:]\n",
    "\n",
    "# Normalize only the selected columns\n",
    "normalized_data = scaler.fit_transform(train_df_dropped[columns_to_normalize])\n",
    "\n",
    "# Combine the normalized and unnormalized columns\n",
    "train_df_normalized = pd.DataFrame(train_df_dropped[columns_to_skip].values, columns=columns_to_skip)\n",
    "train_df_normalized = pd.concat([train_df_normalized, pd.DataFrame(normalized_data, columns=columns_to_normalize)], axis=1)\n",
    "\n",
    "train_df_normalized['RUL'] = train_df_normalized.groupby('engine_id')['cycle'].transform(lambda x: x.max() - x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rolling: 100%|██████████| 28/28 [00:03<00:00,  7.90it/s]\n"
     ]
    }
   ],
   "source": [
    "# from tsfresh.utilities.dataframe_functions import roll_time_series\n",
    "# df_rolled = roll_time_series(train_df_normalized, column_id=\"engine_id\", column_sort=\"cycle\", max_timeshift=30 - 1, min_timeshift=30 - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Melt the DataFrame to stack sensor data for feature extraction\n",
    "sensor_columns = [col for col in train_df_normalized.columns if 'sensor' in col]\n",
    "melted_df = train_df_normalized.melt(\n",
    "    id_vars=[\"engine_id\", \"cycle\"],\n",
    "    value_vars=sensor_columns,\n",
    "    var_name=\"kind\",\n",
    "    value_name=\"value\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feature Extraction: 100%|██████████| 30/30 [00:26<00:00,  1.14it/s]\n"
     ]
    }
   ],
   "source": [
    "extracted_features = extract_features(\n",
    "    melted_df,\n",
    "    column_id=\"engine_id\",\n",
    "    column_sort=\"cycle\",\n",
    "    column_kind=\"kind\",\n",
    "    column_value=\"value\",\n",
    "    # disable_progressbar=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 10962)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extracted_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feature Extraction:   0%|          | 0/30 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feature Extraction: 100%|██████████| 30/30 [00:33<00:00,  1.11s/it]\n"
     ]
    }
   ],
   "source": [
    "# # Feature extraction with TSFRESH\n",
    "# extracted_features = extract_features(train_df_normalized, column_id=\"engine_id\", column_sort=\"cycle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 13311)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extracted_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yshen\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tsfresh\\utilities\\dataframe_functions.py:198: RuntimeWarning: The columns ['setting2__friedrich_coefficients__coeff_0__m_3__r_30'\n",
      " 'setting2__friedrich_coefficients__coeff_1__m_3__r_30'\n",
      " 'setting2__friedrich_coefficients__coeff_2__m_3__r_30'\n",
      " 'setting2__friedrich_coefficients__coeff_3__m_3__r_30'\n",
      " 'setting2__max_langevin_fixed_point__m_3__r_30'\n",
      " 'setting2__query_similarity_count__query_None__threshold_0.0'\n",
      " 'sensor2__query_similarity_count__query_None__threshold_0.0'\n",
      " 'sensor3__query_similarity_count__query_None__threshold_0.0'\n",
      " 'sensor4__query_similarity_count__query_None__threshold_0.0'\n",
      " 'sensor7__query_similarity_count__query_None__threshold_0.0'\n",
      " 'sensor8__friedrich_coefficients__coeff_0__m_3__r_30'\n",
      " 'sensor8__friedrich_coefficients__coeff_1__m_3__r_30'\n",
      " 'sensor8__friedrich_coefficients__coeff_2__m_3__r_30'\n",
      " 'sensor8__friedrich_coefficients__coeff_3__m_3__r_30'\n",
      " 'sensor8__max_langevin_fixed_point__m_3__r_30'\n",
      " 'sensor8__query_similarity_count__query_None__threshold_0.0'\n",
      " 'sensor9__query_similarity_count__query_None__threshold_0.0'\n",
      " 'sensor11__query_similarity_count__query_None__threshold_0.0'\n",
      " 'sensor12__query_similarity_count__query_None__threshold_0.0'\n",
      " 'sensor13__friedrich_coefficients__coeff_0__m_3__r_30'\n",
      " 'sensor13__friedrich_coefficients__coeff_1__m_3__r_30'\n",
      " 'sensor13__friedrich_coefficients__coeff_2__m_3__r_30'\n",
      " 'sensor13__friedrich_coefficients__coeff_3__m_3__r_30'\n",
      " 'sensor13__max_langevin_fixed_point__m_3__r_30'\n",
      " 'sensor13__query_similarity_count__query_None__threshold_0.0'\n",
      " 'sensor14__query_similarity_count__query_None__threshold_0.0'\n",
      " 'sensor15__query_similarity_count__query_None__threshold_0.0'\n",
      " 'sensor17__friedrich_coefficients__coeff_0__m_3__r_30'\n",
      " 'sensor17__friedrich_coefficients__coeff_1__m_3__r_30'\n",
      " 'sensor17__friedrich_coefficients__coeff_2__m_3__r_30'\n",
      " 'sensor17__friedrich_coefficients__coeff_3__m_3__r_30'\n",
      " 'sensor17__max_langevin_fixed_point__m_3__r_30'\n",
      " 'sensor17__query_similarity_count__query_None__threshold_0.0'\n",
      " 'sensor20__query_similarity_count__query_None__threshold_0.0'\n",
      " 'sensor21__query_similarity_count__query_None__threshold_0.0'\n",
      " 'RUL__query_similarity_count__query_None__threshold_0.0'\n",
      " 'setting1__query_similarity_count__query_None__threshold_0.0'] did not have any finite values. Filling with zeros.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>setting2__variance_larger_than_standard_deviation</th>\n",
       "      <th>setting2__has_duplicate_max</th>\n",
       "      <th>setting2__has_duplicate_min</th>\n",
       "      <th>setting2__has_duplicate</th>\n",
       "      <th>setting2__sum_values</th>\n",
       "      <th>setting2__abs_energy</th>\n",
       "      <th>setting2__mean_abs_change</th>\n",
       "      <th>setting2__mean_change</th>\n",
       "      <th>setting2__mean_second_derivative_central</th>\n",
       "      <th>setting2__median</th>\n",
       "      <th>...</th>\n",
       "      <th>setting1__fourier_entropy__bins_5</th>\n",
       "      <th>setting1__fourier_entropy__bins_10</th>\n",
       "      <th>setting1__fourier_entropy__bins_100</th>\n",
       "      <th>setting1__permutation_entropy__dimension_3__tau_1</th>\n",
       "      <th>setting1__permutation_entropy__dimension_4__tau_1</th>\n",
       "      <th>setting1__permutation_entropy__dimension_5__tau_1</th>\n",
       "      <th>setting1__permutation_entropy__dimension_6__tau_1</th>\n",
       "      <th>setting1__permutation_entropy__dimension_7__tau_1</th>\n",
       "      <th>setting1__query_similarity_count__query_None__threshold_0.0</th>\n",
       "      <th>setting1__mean_n_absolute_max__number_of_maxima_7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>103.750000</td>\n",
       "      <td>66.631944</td>\n",
       "      <td>0.277487</td>\n",
       "      <td>0.001745</td>\n",
       "      <td>0.000658</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>...</td>\n",
       "      <td>1.282133</td>\n",
       "      <td>1.906408</td>\n",
       "      <td>3.743622</td>\n",
       "      <td>1.786637</td>\n",
       "      <td>3.136431</td>\n",
       "      <td>4.423680</td>\n",
       "      <td>5.050390</td>\n",
       "      <td>5.203387</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.707718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>164.583333</td>\n",
       "      <td>111.993056</td>\n",
       "      <td>0.276807</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002632</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>...</td>\n",
       "      <td>0.911323</td>\n",
       "      <td>1.506294</td>\n",
       "      <td>3.493835</td>\n",
       "      <td>1.789825</td>\n",
       "      <td>3.137994</td>\n",
       "      <td>4.536881</td>\n",
       "      <td>5.384423</td>\n",
       "      <td>5.593954</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.831691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>87.416667</td>\n",
       "      <td>53.062500</td>\n",
       "      <td>0.261704</td>\n",
       "      <td>-0.002341</td>\n",
       "      <td>0.000942</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.131835</td>\n",
       "      <td>1.737235</td>\n",
       "      <td>3.568428</td>\n",
       "      <td>1.787387</td>\n",
       "      <td>3.138004</td>\n",
       "      <td>4.447641</td>\n",
       "      <td>5.007678</td>\n",
       "      <td>5.153292</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.792282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>105.083333</td>\n",
       "      <td>69.076389</td>\n",
       "      <td>0.266401</td>\n",
       "      <td>-0.003103</td>\n",
       "      <td>0.001337</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>...</td>\n",
       "      <td>0.959535</td>\n",
       "      <td>1.579948</td>\n",
       "      <td>3.562688</td>\n",
       "      <td>1.785316</td>\n",
       "      <td>3.144221</td>\n",
       "      <td>4.474631</td>\n",
       "      <td>5.084010</td>\n",
       "      <td>5.186760</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.771757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>120.916667</td>\n",
       "      <td>68.840278</td>\n",
       "      <td>0.263371</td>\n",
       "      <td>0.000311</td>\n",
       "      <td>0.000624</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>...</td>\n",
       "      <td>1.012771</td>\n",
       "      <td>1.669208</td>\n",
       "      <td>3.687892</td>\n",
       "      <td>1.786792</td>\n",
       "      <td>3.145695</td>\n",
       "      <td>4.562154</td>\n",
       "      <td>5.331026</td>\n",
       "      <td>5.514172</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.774220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>175.583333</td>\n",
       "      <td>109.631944</td>\n",
       "      <td>0.261940</td>\n",
       "      <td>-0.000249</td>\n",
       "      <td>-0.001497</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.665889</td>\n",
       "      <td>1.261253</td>\n",
       "      <td>3.272664</td>\n",
       "      <td>1.790514</td>\n",
       "      <td>3.159163</td>\n",
       "      <td>4.637228</td>\n",
       "      <td>5.592154</td>\n",
       "      <td>5.773887</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.760263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>113.916667</td>\n",
       "      <td>75.854167</td>\n",
       "      <td>0.259536</td>\n",
       "      <td>-0.000829</td>\n",
       "      <td>0.001250</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>...</td>\n",
       "      <td>0.961462</td>\n",
       "      <td>1.609686</td>\n",
       "      <td>3.579486</td>\n",
       "      <td>1.791210</td>\n",
       "      <td>3.132879</td>\n",
       "      <td>4.453631</td>\n",
       "      <td>5.139807</td>\n",
       "      <td>5.256896</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.737274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>69.416667</td>\n",
       "      <td>39.006944</td>\n",
       "      <td>0.248925</td>\n",
       "      <td>-0.000538</td>\n",
       "      <td>-0.000541</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>...</td>\n",
       "      <td>1.148980</td>\n",
       "      <td>1.787885</td>\n",
       "      <td>3.576245</td>\n",
       "      <td>1.788247</td>\n",
       "      <td>3.140119</td>\n",
       "      <td>4.382408</td>\n",
       "      <td>4.903646</td>\n",
       "      <td>4.992151</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.800493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>95.083333</td>\n",
       "      <td>58.451389</td>\n",
       "      <td>0.262228</td>\n",
       "      <td>-0.004076</td>\n",
       "      <td>0.000228</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.945631</td>\n",
       "      <td>1.537181</td>\n",
       "      <td>3.355833</td>\n",
       "      <td>1.785488</td>\n",
       "      <td>3.127782</td>\n",
       "      <td>4.473574</td>\n",
       "      <td>5.040812</td>\n",
       "      <td>5.164152</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.752053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>85.666667</td>\n",
       "      <td>49.125000</td>\n",
       "      <td>0.307370</td>\n",
       "      <td>-0.003350</td>\n",
       "      <td>-0.000210</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>...</td>\n",
       "      <td>0.789457</td>\n",
       "      <td>1.372001</td>\n",
       "      <td>3.375112</td>\n",
       "      <td>1.781986</td>\n",
       "      <td>3.145061</td>\n",
       "      <td>4.469652</td>\n",
       "      <td>5.135241</td>\n",
       "      <td>5.260712</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.770936</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 13311 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     setting2__variance_larger_than_standard_deviation  \\\n",
       "1                                                  0.0   \n",
       "2                                                  0.0   \n",
       "3                                                  0.0   \n",
       "4                                                  0.0   \n",
       "5                                                  0.0   \n",
       "..                                                 ...   \n",
       "96                                                 0.0   \n",
       "97                                                 0.0   \n",
       "98                                                 0.0   \n",
       "99                                                 0.0   \n",
       "100                                                0.0   \n",
       "\n",
       "     setting2__has_duplicate_max  setting2__has_duplicate_min  \\\n",
       "1                            1.0                          1.0   \n",
       "2                            1.0                          1.0   \n",
       "3                            1.0                          1.0   \n",
       "4                            1.0                          1.0   \n",
       "5                            1.0                          1.0   \n",
       "..                           ...                          ...   \n",
       "96                           1.0                          1.0   \n",
       "97                           1.0                          1.0   \n",
       "98                           1.0                          1.0   \n",
       "99                           1.0                          1.0   \n",
       "100                          1.0                          1.0   \n",
       "\n",
       "     setting2__has_duplicate  setting2__sum_values  setting2__abs_energy  \\\n",
       "1                        1.0            103.750000             66.631944   \n",
       "2                        1.0            164.583333            111.993056   \n",
       "3                        1.0             87.416667             53.062500   \n",
       "4                        1.0            105.083333             69.076389   \n",
       "5                        1.0            120.916667             68.840278   \n",
       "..                       ...                   ...                   ...   \n",
       "96                       1.0            175.583333            109.631944   \n",
       "97                       1.0            113.916667             75.854167   \n",
       "98                       1.0             69.416667             39.006944   \n",
       "99                       1.0             95.083333             58.451389   \n",
       "100                      1.0             85.666667             49.125000   \n",
       "\n",
       "     setting2__mean_abs_change  setting2__mean_change  \\\n",
       "1                     0.277487               0.001745   \n",
       "2                     0.276807               0.000000   \n",
       "3                     0.261704              -0.002341   \n",
       "4                     0.266401              -0.003103   \n",
       "5                     0.263371               0.000311   \n",
       "..                         ...                    ...   \n",
       "96                    0.261940              -0.000249   \n",
       "97                    0.259536              -0.000829   \n",
       "98                    0.248925              -0.000538   \n",
       "99                    0.262228              -0.004076   \n",
       "100                   0.307370              -0.003350   \n",
       "\n",
       "     setting2__mean_second_derivative_central  setting2__median  ...  \\\n",
       "1                                    0.000658          0.583333  ...   \n",
       "2                                    0.002632          0.583333  ...   \n",
       "3                                    0.000942          0.500000  ...   \n",
       "4                                    0.001337          0.583333  ...   \n",
       "5                                    0.000624          0.416667  ...   \n",
       "..                                        ...               ...  ...   \n",
       "96                                  -0.001497          0.500000  ...   \n",
       "97                                   0.001250          0.583333  ...   \n",
       "98                                  -0.000541          0.416667  ...   \n",
       "99                                   0.000228          0.500000  ...   \n",
       "100                                 -0.000210          0.416667  ...   \n",
       "\n",
       "     setting1__fourier_entropy__bins_5  setting1__fourier_entropy__bins_10  \\\n",
       "1                             1.282133                            1.906408   \n",
       "2                             0.911323                            1.506294   \n",
       "3                             1.131835                            1.737235   \n",
       "4                             0.959535                            1.579948   \n",
       "5                             1.012771                            1.669208   \n",
       "..                                 ...                                 ...   \n",
       "96                            0.665889                            1.261253   \n",
       "97                            0.961462                            1.609686   \n",
       "98                            1.148980                            1.787885   \n",
       "99                            0.945631                            1.537181   \n",
       "100                           0.789457                            1.372001   \n",
       "\n",
       "     setting1__fourier_entropy__bins_100  \\\n",
       "1                               3.743622   \n",
       "2                               3.493835   \n",
       "3                               3.568428   \n",
       "4                               3.562688   \n",
       "5                               3.687892   \n",
       "..                                   ...   \n",
       "96                              3.272664   \n",
       "97                              3.579486   \n",
       "98                              3.576245   \n",
       "99                              3.355833   \n",
       "100                             3.375112   \n",
       "\n",
       "     setting1__permutation_entropy__dimension_3__tau_1  \\\n",
       "1                                             1.786637   \n",
       "2                                             1.789825   \n",
       "3                                             1.787387   \n",
       "4                                             1.785316   \n",
       "5                                             1.786792   \n",
       "..                                                 ...   \n",
       "96                                            1.790514   \n",
       "97                                            1.791210   \n",
       "98                                            1.788247   \n",
       "99                                            1.785488   \n",
       "100                                           1.781986   \n",
       "\n",
       "     setting1__permutation_entropy__dimension_4__tau_1  \\\n",
       "1                                             3.136431   \n",
       "2                                             3.137994   \n",
       "3                                             3.138004   \n",
       "4                                             3.144221   \n",
       "5                                             3.145695   \n",
       "..                                                 ...   \n",
       "96                                            3.159163   \n",
       "97                                            3.132879   \n",
       "98                                            3.140119   \n",
       "99                                            3.127782   \n",
       "100                                           3.145061   \n",
       "\n",
       "     setting1__permutation_entropy__dimension_5__tau_1  \\\n",
       "1                                             4.423680   \n",
       "2                                             4.536881   \n",
       "3                                             4.447641   \n",
       "4                                             4.474631   \n",
       "5                                             4.562154   \n",
       "..                                                 ...   \n",
       "96                                            4.637228   \n",
       "97                                            4.453631   \n",
       "98                                            4.382408   \n",
       "99                                            4.473574   \n",
       "100                                           4.469652   \n",
       "\n",
       "     setting1__permutation_entropy__dimension_6__tau_1  \\\n",
       "1                                             5.050390   \n",
       "2                                             5.384423   \n",
       "3                                             5.007678   \n",
       "4                                             5.084010   \n",
       "5                                             5.331026   \n",
       "..                                                 ...   \n",
       "96                                            5.592154   \n",
       "97                                            5.139807   \n",
       "98                                            4.903646   \n",
       "99                                            5.040812   \n",
       "100                                           5.135241   \n",
       "\n",
       "     setting1__permutation_entropy__dimension_7__tau_1  \\\n",
       "1                                             5.203387   \n",
       "2                                             5.593954   \n",
       "3                                             5.153292   \n",
       "4                                             5.186760   \n",
       "5                                             5.514172   \n",
       "..                                                 ...   \n",
       "96                                            5.773887   \n",
       "97                                            5.256896   \n",
       "98                                            4.992151   \n",
       "99                                            5.164152   \n",
       "100                                           5.260712   \n",
       "\n",
       "     setting1__query_similarity_count__query_None__threshold_0.0  \\\n",
       "1                                                  0.0             \n",
       "2                                                  0.0             \n",
       "3                                                  0.0             \n",
       "4                                                  0.0             \n",
       "5                                                  0.0             \n",
       "..                                                 ...             \n",
       "96                                                 0.0             \n",
       "97                                                 0.0             \n",
       "98                                                 0.0             \n",
       "99                                                 0.0             \n",
       "100                                                0.0             \n",
       "\n",
       "     setting1__mean_n_absolute_max__number_of_maxima_7  \n",
       "1                                             0.707718  \n",
       "2                                             0.831691  \n",
       "3                                             0.792282  \n",
       "4                                             0.771757  \n",
       "5                                             0.774220  \n",
       "..                                                 ...  \n",
       "96                                            0.760263  \n",
       "97                                            0.737274  \n",
       "98                                            0.800493  \n",
       "99                                            0.752053  \n",
       "100                                           0.770936  \n",
       "\n",
       "[100 rows x 13311 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filling up NaN as recommended by TSFRESH\n",
    "impute(extracted_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = train_df_normalized[\"RUL\"].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "X and y must contain the same number of samples.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m features_filtered \u001b[38;5;241m=\u001b[39m \u001b[43mselect_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mextracted_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\yshen\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tsfresh\\feature_selection\\selection.py:154\u001b[0m, in \u001b[0;36mselect_features\u001b[1;34m(X, y, test_for_binary_target_binary_feature, test_for_binary_target_real_feature, test_for_real_target_binary_feature, test_for_real_target_real_feature, fdr_level, hypotheses_independent, n_jobs, show_warnings, chunksize, ml_task, multiclass, n_significant)\u001b[0m\n\u001b[0;32m    150\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(y, (pd\u001b[38;5;241m.\u001b[39mSeries, np\u001b[38;5;241m.\u001b[39mndarray)), (\n\u001b[0;32m    151\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe type of target vector y must be one of: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpandas.Series, numpy.ndarray\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    152\u001b[0m )\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(y) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my must contain at least two samples.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 154\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(X) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(y), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX and y must contain the same number of samples.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    155\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[0;32m    156\u001b[0m     \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mset\u001b[39m(y)) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    157\u001b[0m ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFeature selection is only possible if more than 1 label/class is provided\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    159\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(y, pd\u001b[38;5;241m.\u001b[39mSeries) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mset\u001b[39m(X\u001b[38;5;241m.\u001b[39mindex) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mset\u001b[39m(y\u001b[38;5;241m.\u001b[39mindex):\n",
      "\u001b[1;31mAssertionError\u001b[0m: X and y must contain the same number of samples."
     ]
    }
   ],
   "source": [
    "features_filtered = select_features(extracted_features, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yshen\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\_array_api.py:695: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmin(X, axis=axis))\n",
      "c:\\Users\\yshen\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\_array_api.py:712: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmax(X, axis=axis))\n",
      "C:\\Users\\yshen\\AppData\\Local\\Temp\\ipykernel_21352\\3581742827.py:43: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  melted_df['value'].fillna(melted_df['value'].mean(), inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique_id         0\n",
      "engine_id         0\n",
      "cycle             0\n",
      "kind              0\n",
      "value        103155\n",
      "dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feature Extraction:   0%|          | 0/30 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tsfresh import extract_features\n",
    "from tsfresh.utilities.dataframe_functions import impute\n",
    "\n",
    "# Load the CMAPSS dataset\n",
    "column_names = ['engine_id', 'cycle', 'setting1', 'setting2', 'setting3'] + [f'sensor{i}' for i in range(1, 27)]\n",
    "train_df = pd.read_csv('./dataset/train_FD001.txt', sep='\\s+', header=None, names=column_names)\n",
    "\n",
    "# Drop columns with NaNs and uninformative columns\n",
    "columns_to_drop = [\"setting3\", \"sensor1\", \"sensor5\", \"sensor6\", \"sensor10\", \"sensor16\", \"sensor18\", \"sensor19\"]\n",
    "train_df_dropped = train_df.drop(columns=columns_to_drop)\n",
    "\n",
    "# Normalize the data\n",
    "scaler = MinMaxScaler()\n",
    "columns_to_skip = train_df_dropped.columns[:2]  # Skip 'engine_id' and 'cycle'\n",
    "columns_to_normalize = train_df_dropped.columns[2:]\n",
    "normalized_data = scaler.fit_transform(train_df_dropped[columns_to_normalize])\n",
    "train_df_normalized = pd.concat([train_df_dropped[columns_to_skip].reset_index(drop=True), \n",
    "                                 pd.DataFrame(normalized_data, columns=columns_to_normalize)], axis=1)\n",
    "\n",
    "# Calculate RUL and add it as a target column\n",
    "train_df_normalized['RUL'] = train_df_normalized.groupby('engine_id')['cycle'].transform(lambda x: x.max() - x)\n",
    "\n",
    "# Create a unique ID for each cycle by combining engine_id and cycle\n",
    "train_df_normalized['unique_id'] = train_df_normalized['engine_id'].astype(str) + '_' + train_df_normalized['cycle'].astype(str)\n",
    "\n",
    "# Melt the DataFrame to stack sensor data for feature extraction at each cycle\n",
    "sensor_columns = [col for col in train_df_normalized.columns if 'sensor' in col]\n",
    "melted_df = train_df_normalized.melt(\n",
    "    id_vars=[\"unique_id\", \"engine_id\", \"cycle\"],\n",
    "    value_vars=sensor_columns,\n",
    "    var_name=\"kind\",   # Identifies the sensor type\n",
    "    value_name=\"value\" # Contains the sensor reading values\n",
    ")\n",
    "\n",
    "# Step 1: Check for missing values in the melted DataFrame\n",
    "print(melted_df.isnull().sum())\n",
    "\n",
    "# Step 2: Impute or drop missing values\n",
    "# Option 1: Fill NaNs with the mean of each sensor's values\n",
    "melted_df['value'].fillna(melted_df['value'].mean(), inplace=True)\n",
    "\n",
    "# Option 2: Drop rows with NaN values in 'value' (use only if dropping is acceptable)\n",
    "# melted_df = melted_df.dropna(subset=['value'])\n",
    "# Extract features for each cycle using the unique_id as the identifier\n",
    "extracted_features = extract_features(\n",
    "    melted_df,\n",
    "    column_id=\"unique_id\",\n",
    "    column_sort=\"cycle\",\n",
    "    column_kind=\"kind\",\n",
    "    column_value=\"value\",\n",
    "    disable_progressbar=False\n",
    ")\n",
    "\n",
    "# Impute any missing values in the features\n",
    "impute(extracted_features)\n",
    "\n",
    "# Display extracted features\n",
    "print(extracted_features.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yshen\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\_array_api.py:695: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmin(X, axis=axis))\n",
      "c:\\Users\\yshen\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\_array_api.py:712: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmax(X, axis=axis))\n",
      "WARNING:tsfresh.feature_extraction.settings:Dependency not available for matrix_profile, this feature will be disabled!\n",
      "Feature Extraction: 100%|██████████| 30/30 [00:56<00:00,  1.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         sensor11__sum_values  sensor11__median  sensor11__mean  \\\n",
      "100_1                0.303571          0.303571        0.303571   \n",
      "100_10               0.244048          0.244048        0.244048   \n",
      "100_100              0.494048          0.494048        0.494048   \n",
      "100_101              0.476190          0.476190        0.476190   \n",
      "100_102              0.440476          0.440476        0.440476   \n",
      "\n",
      "         sensor11__length  sensor11__standard_deviation  sensor11__variance  \\\n",
      "100_1                 1.0                           0.0                 0.0   \n",
      "100_10                1.0                           0.0                 0.0   \n",
      "100_100               1.0                           0.0                 0.0   \n",
      "100_101               1.0                           0.0                 0.0   \n",
      "100_102               1.0                           0.0                 0.0   \n",
      "\n",
      "         sensor11__root_mean_square  sensor11__maximum  \\\n",
      "100_1                      0.303571           0.303571   \n",
      "100_10                     0.244048           0.244048   \n",
      "100_100                    0.494048           0.494048   \n",
      "100_101                    0.476190           0.476190   \n",
      "100_102                    0.440476           0.440476   \n",
      "\n",
      "         sensor11__absolute_maximum  sensor11__minimum  ...  \\\n",
      "100_1                      0.303571           0.303571  ...   \n",
      "100_10                     0.244048           0.244048  ...   \n",
      "100_100                    0.494048           0.494048  ...   \n",
      "100_101                    0.476190           0.476190  ...   \n",
      "100_102                    0.440476           0.440476  ...   \n",
      "\n",
      "         sensor9__sum_values  sensor9__median  sensor9__mean  sensor9__length  \\\n",
      "100_1               0.153639         0.153639       0.153639              1.0   \n",
      "100_10              0.188953         0.188953       0.188953              1.0   \n",
      "100_100             0.202683         0.202683       0.202683              1.0   \n",
      "100_101             0.184735         0.184735       0.184735              1.0   \n",
      "100_102             0.203536         0.203536       0.203536              1.0   \n",
      "\n",
      "         sensor9__standard_deviation  sensor9__variance  \\\n",
      "100_1                            0.0                0.0   \n",
      "100_10                           0.0                0.0   \n",
      "100_100                          0.0                0.0   \n",
      "100_101                          0.0                0.0   \n",
      "100_102                          0.0                0.0   \n",
      "\n",
      "         sensor9__root_mean_square  sensor9__maximum  \\\n",
      "100_1                     0.153639          0.153639   \n",
      "100_10                    0.188953          0.188953   \n",
      "100_100                   0.202683          0.202683   \n",
      "100_101                   0.184735          0.184735   \n",
      "100_102                   0.203536          0.203536   \n",
      "\n",
      "         sensor9__absolute_maximum  sensor9__minimum  \n",
      "100_1                     0.153639          0.153639  \n",
      "100_10                    0.188953          0.188953  \n",
      "100_100                   0.202683          0.202683  \n",
      "100_101                   0.184735          0.184735  \n",
      "100_102                   0.203536          0.203536  \n",
      "\n",
      "[5 rows x 190 columns]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tsfresh import extract_features\n",
    "from tsfresh.utilities.dataframe_functions import impute\n",
    "from tsfresh.feature_extraction import MinimalFCParameters\n",
    "import logging\n",
    "\n",
    "# Set up logging for debugging purposes\n",
    "logging.basicConfig(level=logging.INFO)  # Set logging level to show details\n",
    "\n",
    "# Load and preprocess the data\n",
    "column_names = ['engine_id', 'cycle', 'setting1', 'setting2', 'setting3'] + [f'sensor{i}' for i in range(1, 27)]\n",
    "train_df = pd.read_csv('./dataset/train_FD001.txt', sep='\\s+', header=None, names=column_names)\n",
    "\n",
    "# Drop uninformative columns\n",
    "columns_to_drop = [\"setting3\", \"sensor1\", \"sensor5\", \"sensor6\", \"sensor10\", \"sensor16\", \"sensor18\", \"sensor19\"]\n",
    "train_df_dropped = train_df.drop(columns=columns_to_drop)\n",
    "\n",
    "# Normalize data\n",
    "scaler = MinMaxScaler()\n",
    "columns_to_skip = train_df_dropped.columns[:2]\n",
    "columns_to_normalize = train_df_dropped.columns[2:]\n",
    "normalized_data = scaler.fit_transform(train_df_dropped[columns_to_normalize])\n",
    "train_df_normalized = pd.concat([train_df_dropped[columns_to_skip].reset_index(drop=True), \n",
    "                                 pd.DataFrame(normalized_data, columns=columns_to_normalize)], axis=1)\n",
    "\n",
    "# Calculate RUL\n",
    "train_df_normalized['RUL'] = train_df_normalized.groupby('engine_id')['cycle'].transform(lambda x: x.max() - x)\n",
    "\n",
    "# Sensor columns for interpolation\n",
    "sensor_columns = [col for col in train_df_normalized.columns if 'sensor' in col]\n",
    "\n",
    "# Step 1: Interpolate missing values for each sensor column (row-wise interpolation)\n",
    "train_df_normalized[sensor_columns] = train_df_normalized[sensor_columns].interpolate(method='linear', limit_direction='both')\n",
    "\n",
    "# Step 2: For any remaining NaNs, fill with the median of each column as a fallback\n",
    "train_df_normalized[sensor_columns] = train_df_normalized[sensor_columns].fillna(train_df_normalized[sensor_columns].median())\n",
    "\n",
    "# Step 3: Ensure no NaN values remain by filling any final NaNs with 0 as a last resort\n",
    "train_df_normalized[sensor_columns] = train_df_normalized[sensor_columns].fillna(0)\n",
    "\n",
    "# Create a unique ID for each cycle (for row-by-row extraction)\n",
    "train_df_normalized['unique_id'] = train_df_normalized['engine_id'].astype(str) + '_' + train_df_normalized['cycle'].astype(str)\n",
    "\n",
    "# Melt the DataFrame for feature extraction with tsfresh\n",
    "melted_df = train_df_normalized.melt(\n",
    "    id_vars=[\"unique_id\", \"engine_id\", \"cycle\"],\n",
    "    value_vars=sensor_columns,\n",
    "    var_name=\"kind\",\n",
    "    value_name=\"value\"\n",
    ")\n",
    "\n",
    "# Verify that there are no NaN values in melted_df before extraction\n",
    "assert melted_df['value'].isnull().sum() == 0, \"NaN values found in 'value' column after cleaning!\"\n",
    "\n",
    "# Minimal feature extraction for debugging\n",
    "extracted_features = extract_features(\n",
    "    melted_df,\n",
    "    column_id=\"unique_id\",\n",
    "    column_sort=\"cycle\",\n",
    "    column_kind=\"kind\",\n",
    "    column_value=\"value\",\n",
    "    default_fc_parameters=MinimalFCParameters(),  # Minimal feature set for testing\n",
    "    disable_progressbar=False,\n",
    "    show_warnings=True\n",
    ")\n",
    "\n",
    "# Impute any remaining missing values in extracted features\n",
    "impute(extracted_features)\n",
    "\n",
    "# Display the extracted features\n",
    "print(extracted_features.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20631, 190)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extracted_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yshen\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\_array_api.py:695: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmin(X, axis=axis))\n",
      "c:\\Users\\yshen\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\_array_api.py:712: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmax(X, axis=axis))\n",
      "Rolling: 100%|██████████| 28/28 [00:03<00:00,  7.93it/s]\n",
      "C:\\Users\\yshen\\AppData\\Local\\Temp\\ipykernel_22004\\307786759.py:54: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  melted_df['value'].fillna(0, inplace=True)\n",
      "WARNING:tsfresh.feature_extraction.settings:Dependency not available for matrix_profile, this feature will be disabled!\n",
      "Feature Extraction: 100%|██████████| 30/30 [02:39<00:00,  5.33s/it]\n",
      "c:\\Users\\yshen\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tsfresh\\utilities\\dataframe_functions.py:198: RuntimeWarning: The columns ['sensor13__friedrich_coefficients__coeff_0__m_3__r_30'\n",
      " 'sensor13__friedrich_coefficients__coeff_1__m_3__r_30'\n",
      " 'sensor13__friedrich_coefficients__coeff_2__m_3__r_30'\n",
      " 'sensor13__friedrich_coefficients__coeff_3__m_3__r_30'\n",
      " 'sensor13__max_langevin_fixed_point__m_3__r_30'\n",
      " 'sensor13__query_similarity_count__query_None__threshold_0.0'\n",
      " 'sensor14__query_similarity_count__query_None__threshold_0.0'\n",
      " 'sensor15__query_similarity_count__query_None__threshold_0.0'\n",
      " 'sensor17__friedrich_coefficients__coeff_0__m_3__r_30'\n",
      " 'sensor17__friedrich_coefficients__coeff_1__m_3__r_30'\n",
      " 'sensor17__friedrich_coefficients__coeff_2__m_3__r_30'\n",
      " 'sensor17__friedrich_coefficients__coeff_3__m_3__r_30'\n",
      " 'sensor17__max_langevin_fixed_point__m_3__r_30'\n",
      " 'sensor17__query_similarity_count__query_None__threshold_0.0'\n",
      " 'sensor2__query_similarity_count__query_None__threshold_0.0'\n",
      " 'sensor20__query_similarity_count__query_None__threshold_0.0'\n",
      " 'sensor21__query_similarity_count__query_None__threshold_0.0'\n",
      " 'sensor22__variation_coefficient' 'sensor22__benford_correlation'\n",
      " 'sensor22__autocorrelation__lag_0' 'sensor22__autocorrelation__lag_1'\n",
      " 'sensor22__autocorrelation__lag_2' 'sensor22__autocorrelation__lag_3'\n",
      " 'sensor22__autocorrelation__lag_4' 'sensor22__autocorrelation__lag_5'\n",
      " 'sensor22__autocorrelation__lag_6' 'sensor22__autocorrelation__lag_7'\n",
      " 'sensor22__autocorrelation__lag_8' 'sensor22__autocorrelation__lag_9'\n",
      " 'sensor22__partial_autocorrelation__lag_1'\n",
      " 'sensor22__partial_autocorrelation__lag_2'\n",
      " 'sensor22__partial_autocorrelation__lag_3'\n",
      " 'sensor22__partial_autocorrelation__lag_4'\n",
      " 'sensor22__partial_autocorrelation__lag_5'\n",
      " 'sensor22__partial_autocorrelation__lag_6'\n",
      " 'sensor22__partial_autocorrelation__lag_7'\n",
      " 'sensor22__partial_autocorrelation__lag_8'\n",
      " 'sensor22__partial_autocorrelation__lag_9'\n",
      " 'sensor22__index_mass_quantile__q_0.1'\n",
      " 'sensor22__index_mass_quantile__q_0.2'\n",
      " 'sensor22__index_mass_quantile__q_0.3'\n",
      " 'sensor22__index_mass_quantile__q_0.4'\n",
      " 'sensor22__index_mass_quantile__q_0.6'\n",
      " 'sensor22__index_mass_quantile__q_0.7'\n",
      " 'sensor22__index_mass_quantile__q_0.8'\n",
      " 'sensor22__index_mass_quantile__q_0.9'\n",
      " 'sensor22__fft_aggregated__aggtype_\"centroid\"'\n",
      " 'sensor22__fft_aggregated__aggtype_\"variance\"'\n",
      " 'sensor22__fft_aggregated__aggtype_\"skew\"'\n",
      " 'sensor22__fft_aggregated__aggtype_\"kurtosis\"'\n",
      " 'sensor22__friedrich_coefficients__coeff_0__m_3__r_30'\n",
      " 'sensor22__friedrich_coefficients__coeff_1__m_3__r_30'\n",
      " 'sensor22__friedrich_coefficients__coeff_2__m_3__r_30'\n",
      " 'sensor22__friedrich_coefficients__coeff_3__m_3__r_30'\n",
      " 'sensor22__max_langevin_fixed_point__m_3__r_30'\n",
      " 'sensor22__augmented_dickey_fuller__attr_\"teststat\"__autolag_\"AIC\"'\n",
      " 'sensor22__augmented_dickey_fuller__attr_\"pvalue\"__autolag_\"AIC\"'\n",
      " 'sensor22__augmented_dickey_fuller__attr_\"usedlag\"__autolag_\"AIC\"'\n",
      " 'sensor22__energy_ratio_by_chunks__num_segments_10__segment_focus_0'\n",
      " 'sensor22__energy_ratio_by_chunks__num_segments_10__segment_focus_1'\n",
      " 'sensor22__energy_ratio_by_chunks__num_segments_10__segment_focus_2'\n",
      " 'sensor22__energy_ratio_by_chunks__num_segments_10__segment_focus_3'\n",
      " 'sensor22__energy_ratio_by_chunks__num_segments_10__segment_focus_4'\n",
      " 'sensor22__energy_ratio_by_chunks__num_segments_10__segment_focus_5'\n",
      " 'sensor22__energy_ratio_by_chunks__num_segments_10__segment_focus_6'\n",
      " 'sensor22__energy_ratio_by_chunks__num_segments_10__segment_focus_7'\n",
      " 'sensor22__energy_ratio_by_chunks__num_segments_10__segment_focus_8'\n",
      " 'sensor22__energy_ratio_by_chunks__num_segments_10__segment_focus_9'\n",
      " 'sensor22__fourier_entropy__bins_2' 'sensor22__fourier_entropy__bins_3'\n",
      " 'sensor22__fourier_entropy__bins_5' 'sensor22__fourier_entropy__bins_10'\n",
      " 'sensor22__fourier_entropy__bins_100'\n",
      " 'sensor22__query_similarity_count__query_None__threshold_0.0'\n",
      " 'sensor23__variation_coefficient' 'sensor23__benford_correlation'\n",
      " 'sensor23__autocorrelation__lag_0' 'sensor23__autocorrelation__lag_1'\n",
      " 'sensor23__autocorrelation__lag_2' 'sensor23__autocorrelation__lag_3'\n",
      " 'sensor23__autocorrelation__lag_4' 'sensor23__autocorrelation__lag_5'\n",
      " 'sensor23__autocorrelation__lag_6' 'sensor23__autocorrelation__lag_7'\n",
      " 'sensor23__autocorrelation__lag_8' 'sensor23__autocorrelation__lag_9'\n",
      " 'sensor23__partial_autocorrelation__lag_1'\n",
      " 'sensor23__partial_autocorrelation__lag_2'\n",
      " 'sensor23__partial_autocorrelation__lag_3'\n",
      " 'sensor23__partial_autocorrelation__lag_4'\n",
      " 'sensor23__partial_autocorrelation__lag_5'\n",
      " 'sensor23__partial_autocorrelation__lag_6'\n",
      " 'sensor23__partial_autocorrelation__lag_7'\n",
      " 'sensor23__partial_autocorrelation__lag_8'\n",
      " 'sensor23__partial_autocorrelation__lag_9'\n",
      " 'sensor23__index_mass_quantile__q_0.1'\n",
      " 'sensor23__index_mass_quantile__q_0.2'\n",
      " 'sensor23__index_mass_quantile__q_0.3'\n",
      " 'sensor23__index_mass_quantile__q_0.4'\n",
      " 'sensor23__index_mass_quantile__q_0.6'\n",
      " 'sensor23__index_mass_quantile__q_0.7'\n",
      " 'sensor23__index_mass_quantile__q_0.8'\n",
      " 'sensor23__index_mass_quantile__q_0.9'\n",
      " 'sensor23__fft_aggregated__aggtype_\"centroid\"'\n",
      " 'sensor23__fft_aggregated__aggtype_\"variance\"'\n",
      " 'sensor23__fft_aggregated__aggtype_\"skew\"'\n",
      " 'sensor23__fft_aggregated__aggtype_\"kurtosis\"'\n",
      " 'sensor23__friedrich_coefficients__coeff_0__m_3__r_30'\n",
      " 'sensor23__friedrich_coefficients__coeff_1__m_3__r_30'\n",
      " 'sensor23__friedrich_coefficients__coeff_2__m_3__r_30'\n",
      " 'sensor23__friedrich_coefficients__coeff_3__m_3__r_30'\n",
      " 'sensor23__max_langevin_fixed_point__m_3__r_30'\n",
      " 'sensor23__augmented_dickey_fuller__attr_\"teststat\"__autolag_\"AIC\"'\n",
      " 'sensor23__augmented_dickey_fuller__attr_\"pvalue\"__autolag_\"AIC\"'\n",
      " 'sensor23__augmented_dickey_fuller__attr_\"usedlag\"__autolag_\"AIC\"'\n",
      " 'sensor23__energy_ratio_by_chunks__num_segments_10__segment_focus_0'\n",
      " 'sensor23__energy_ratio_by_chunks__num_segments_10__segment_focus_1'\n",
      " 'sensor23__energy_ratio_by_chunks__num_segments_10__segment_focus_2'\n",
      " 'sensor23__energy_ratio_by_chunks__num_segments_10__segment_focus_3'\n",
      " 'sensor23__energy_ratio_by_chunks__num_segments_10__segment_focus_4'\n",
      " 'sensor23__energy_ratio_by_chunks__num_segments_10__segment_focus_5'\n",
      " 'sensor23__energy_ratio_by_chunks__num_segments_10__segment_focus_6'\n",
      " 'sensor23__energy_ratio_by_chunks__num_segments_10__segment_focus_7'\n",
      " 'sensor23__energy_ratio_by_chunks__num_segments_10__segment_focus_8'\n",
      " 'sensor23__energy_ratio_by_chunks__num_segments_10__segment_focus_9'\n",
      " 'sensor23__fourier_entropy__bins_2' 'sensor23__fourier_entropy__bins_3'\n",
      " 'sensor23__fourier_entropy__bins_5' 'sensor23__fourier_entropy__bins_10'\n",
      " 'sensor23__fourier_entropy__bins_100'\n",
      " 'sensor23__query_similarity_count__query_None__threshold_0.0'\n",
      " 'sensor24__variation_coefficient' 'sensor24__benford_correlation'\n",
      " 'sensor24__autocorrelation__lag_0' 'sensor24__autocorrelation__lag_1'\n",
      " 'sensor24__autocorrelation__lag_2' 'sensor24__autocorrelation__lag_3'\n",
      " 'sensor24__autocorrelation__lag_4' 'sensor24__autocorrelation__lag_5'\n",
      " 'sensor24__autocorrelation__lag_6' 'sensor24__autocorrelation__lag_7'\n",
      " 'sensor24__autocorrelation__lag_8' 'sensor24__autocorrelation__lag_9'\n",
      " 'sensor24__partial_autocorrelation__lag_1'\n",
      " 'sensor24__partial_autocorrelation__lag_2'\n",
      " 'sensor24__partial_autocorrelation__lag_3'\n",
      " 'sensor24__partial_autocorrelation__lag_4'\n",
      " 'sensor24__partial_autocorrelation__lag_5'\n",
      " 'sensor24__partial_autocorrelation__lag_6'\n",
      " 'sensor24__partial_autocorrelation__lag_7'\n",
      " 'sensor24__partial_autocorrelation__lag_8'\n",
      " 'sensor24__partial_autocorrelation__lag_9'\n",
      " 'sensor24__index_mass_quantile__q_0.1'\n",
      " 'sensor24__index_mass_quantile__q_0.2'\n",
      " 'sensor24__index_mass_quantile__q_0.3'\n",
      " 'sensor24__index_mass_quantile__q_0.4'\n",
      " 'sensor24__index_mass_quantile__q_0.6'\n",
      " 'sensor24__index_mass_quantile__q_0.7'\n",
      " 'sensor24__index_mass_quantile__q_0.8'\n",
      " 'sensor24__index_mass_quantile__q_0.9'\n",
      " 'sensor24__fft_aggregated__aggtype_\"centroid\"'\n",
      " 'sensor24__fft_aggregated__aggtype_\"variance\"'\n",
      " 'sensor24__fft_aggregated__aggtype_\"skew\"'\n",
      " 'sensor24__fft_aggregated__aggtype_\"kurtosis\"'\n",
      " 'sensor24__friedrich_coefficients__coeff_0__m_3__r_30'\n",
      " 'sensor24__friedrich_coefficients__coeff_1__m_3__r_30'\n",
      " 'sensor24__friedrich_coefficients__coeff_2__m_3__r_30'\n",
      " 'sensor24__friedrich_coefficients__coeff_3__m_3__r_30'\n",
      " 'sensor24__max_langevin_fixed_point__m_3__r_30'\n",
      " 'sensor24__augmented_dickey_fuller__attr_\"teststat\"__autolag_\"AIC\"'\n",
      " 'sensor24__augmented_dickey_fuller__attr_\"pvalue\"__autolag_\"AIC\"'\n",
      " 'sensor24__augmented_dickey_fuller__attr_\"usedlag\"__autolag_\"AIC\"'\n",
      " 'sensor24__energy_ratio_by_chunks__num_segments_10__segment_focus_0'\n",
      " 'sensor24__energy_ratio_by_chunks__num_segments_10__segment_focus_1'\n",
      " 'sensor24__energy_ratio_by_chunks__num_segments_10__segment_focus_2'\n",
      " 'sensor24__energy_ratio_by_chunks__num_segments_10__segment_focus_3'\n",
      " 'sensor24__energy_ratio_by_chunks__num_segments_10__segment_focus_4'\n",
      " 'sensor24__energy_ratio_by_chunks__num_segments_10__segment_focus_5'\n",
      " 'sensor24__energy_ratio_by_chunks__num_segments_10__segment_focus_6'\n",
      " 'sensor24__energy_ratio_by_chunks__num_segments_10__segment_focus_7'\n",
      " 'sensor24__energy_ratio_by_chunks__num_segments_10__segment_focus_8'\n",
      " 'sensor24__energy_ratio_by_chunks__num_segments_10__segment_focus_9'\n",
      " 'sensor24__fourier_entropy__bins_2' 'sensor24__fourier_entropy__bins_3'\n",
      " 'sensor24__fourier_entropy__bins_5' 'sensor24__fourier_entropy__bins_10'\n",
      " 'sensor24__fourier_entropy__bins_100'\n",
      " 'sensor24__query_similarity_count__query_None__threshold_0.0'\n",
      " 'sensor25__variation_coefficient' 'sensor25__benford_correlation'\n",
      " 'sensor25__autocorrelation__lag_0' 'sensor25__autocorrelation__lag_1'\n",
      " 'sensor25__autocorrelation__lag_2' 'sensor25__autocorrelation__lag_3'\n",
      " 'sensor25__autocorrelation__lag_4' 'sensor25__autocorrelation__lag_5'\n",
      " 'sensor25__autocorrelation__lag_6' 'sensor25__autocorrelation__lag_7'\n",
      " 'sensor25__autocorrelation__lag_8' 'sensor25__autocorrelation__lag_9'\n",
      " 'sensor25__partial_autocorrelation__lag_1'\n",
      " 'sensor25__partial_autocorrelation__lag_2'\n",
      " 'sensor25__partial_autocorrelation__lag_3'\n",
      " 'sensor25__partial_autocorrelation__lag_4'\n",
      " 'sensor25__partial_autocorrelation__lag_5'\n",
      " 'sensor25__partial_autocorrelation__lag_6'\n",
      " 'sensor25__partial_autocorrelation__lag_7'\n",
      " 'sensor25__partial_autocorrelation__lag_8'\n",
      " 'sensor25__partial_autocorrelation__lag_9'\n",
      " 'sensor25__index_mass_quantile__q_0.1'\n",
      " 'sensor25__index_mass_quantile__q_0.2'\n",
      " 'sensor25__index_mass_quantile__q_0.3'\n",
      " 'sensor25__index_mass_quantile__q_0.4'\n",
      " 'sensor25__index_mass_quantile__q_0.6'\n",
      " 'sensor25__index_mass_quantile__q_0.7'\n",
      " 'sensor25__index_mass_quantile__q_0.8'\n",
      " 'sensor25__index_mass_quantile__q_0.9'\n",
      " 'sensor25__fft_aggregated__aggtype_\"centroid\"'\n",
      " 'sensor25__fft_aggregated__aggtype_\"variance\"'\n",
      " 'sensor25__fft_aggregated__aggtype_\"skew\"'\n",
      " 'sensor25__fft_aggregated__aggtype_\"kurtosis\"'\n",
      " 'sensor25__friedrich_coefficients__coeff_0__m_3__r_30'\n",
      " 'sensor25__friedrich_coefficients__coeff_1__m_3__r_30'\n",
      " 'sensor25__friedrich_coefficients__coeff_2__m_3__r_30'\n",
      " 'sensor25__friedrich_coefficients__coeff_3__m_3__r_30'\n",
      " 'sensor25__max_langevin_fixed_point__m_3__r_30'\n",
      " 'sensor25__augmented_dickey_fuller__attr_\"teststat\"__autolag_\"AIC\"'\n",
      " 'sensor25__augmented_dickey_fuller__attr_\"pvalue\"__autolag_\"AIC\"'\n",
      " 'sensor25__augmented_dickey_fuller__attr_\"usedlag\"__autolag_\"AIC\"'\n",
      " 'sensor25__energy_ratio_by_chunks__num_segments_10__segment_focus_0'\n",
      " 'sensor25__energy_ratio_by_chunks__num_segments_10__segment_focus_1'\n",
      " 'sensor25__energy_ratio_by_chunks__num_segments_10__segment_focus_2'\n",
      " 'sensor25__energy_ratio_by_chunks__num_segments_10__segment_focus_3'\n",
      " 'sensor25__energy_ratio_by_chunks__num_segments_10__segment_focus_4'\n",
      " 'sensor25__energy_ratio_by_chunks__num_segments_10__segment_focus_5'\n",
      " 'sensor25__energy_ratio_by_chunks__num_segments_10__segment_focus_6'\n",
      " 'sensor25__energy_ratio_by_chunks__num_segments_10__segment_focus_7'\n",
      " 'sensor25__energy_ratio_by_chunks__num_segments_10__segment_focus_8'\n",
      " 'sensor25__energy_ratio_by_chunks__num_segments_10__segment_focus_9'\n",
      " 'sensor25__fourier_entropy__bins_2' 'sensor25__fourier_entropy__bins_3'\n",
      " 'sensor25__fourier_entropy__bins_5' 'sensor25__fourier_entropy__bins_10'\n",
      " 'sensor25__fourier_entropy__bins_100'\n",
      " 'sensor25__query_similarity_count__query_None__threshold_0.0'\n",
      " 'sensor26__variation_coefficient' 'sensor26__benford_correlation'\n",
      " 'sensor26__autocorrelation__lag_0' 'sensor26__autocorrelation__lag_1'\n",
      " 'sensor26__autocorrelation__lag_2' 'sensor26__autocorrelation__lag_3'\n",
      " 'sensor26__autocorrelation__lag_4' 'sensor26__autocorrelation__lag_5'\n",
      " 'sensor26__autocorrelation__lag_6' 'sensor26__autocorrelation__lag_7'\n",
      " 'sensor26__autocorrelation__lag_8' 'sensor26__autocorrelation__lag_9'\n",
      " 'sensor26__partial_autocorrelation__lag_1'\n",
      " 'sensor26__partial_autocorrelation__lag_2'\n",
      " 'sensor26__partial_autocorrelation__lag_3'\n",
      " 'sensor26__partial_autocorrelation__lag_4'\n",
      " 'sensor26__partial_autocorrelation__lag_5'\n",
      " 'sensor26__partial_autocorrelation__lag_6'\n",
      " 'sensor26__partial_autocorrelation__lag_7'\n",
      " 'sensor26__partial_autocorrelation__lag_8'\n",
      " 'sensor26__partial_autocorrelation__lag_9'\n",
      " 'sensor26__index_mass_quantile__q_0.1'\n",
      " 'sensor26__index_mass_quantile__q_0.2'\n",
      " 'sensor26__index_mass_quantile__q_0.3'\n",
      " 'sensor26__index_mass_quantile__q_0.4'\n",
      " 'sensor26__index_mass_quantile__q_0.6'\n",
      " 'sensor26__index_mass_quantile__q_0.7'\n",
      " 'sensor26__index_mass_quantile__q_0.8'\n",
      " 'sensor26__index_mass_quantile__q_0.9'\n",
      " 'sensor26__fft_aggregated__aggtype_\"centroid\"'\n",
      " 'sensor26__fft_aggregated__aggtype_\"variance\"'\n",
      " 'sensor26__fft_aggregated__aggtype_\"skew\"'\n",
      " 'sensor26__fft_aggregated__aggtype_\"kurtosis\"'\n",
      " 'sensor26__friedrich_coefficients__coeff_0__m_3__r_30'\n",
      " 'sensor26__friedrich_coefficients__coeff_1__m_3__r_30'\n",
      " 'sensor26__friedrich_coefficients__coeff_2__m_3__r_30'\n",
      " 'sensor26__friedrich_coefficients__coeff_3__m_3__r_30'\n",
      " 'sensor26__max_langevin_fixed_point__m_3__r_30'\n",
      " 'sensor26__augmented_dickey_fuller__attr_\"teststat\"__autolag_\"AIC\"'\n",
      " 'sensor26__augmented_dickey_fuller__attr_\"pvalue\"__autolag_\"AIC\"'\n",
      " 'sensor26__augmented_dickey_fuller__attr_\"usedlag\"__autolag_\"AIC\"'\n",
      " 'sensor26__energy_ratio_by_chunks__num_segments_10__segment_focus_0'\n",
      " 'sensor26__energy_ratio_by_chunks__num_segments_10__segment_focus_1'\n",
      " 'sensor26__energy_ratio_by_chunks__num_segments_10__segment_focus_2'\n",
      " 'sensor26__energy_ratio_by_chunks__num_segments_10__segment_focus_3'\n",
      " 'sensor26__energy_ratio_by_chunks__num_segments_10__segment_focus_4'\n",
      " 'sensor26__energy_ratio_by_chunks__num_segments_10__segment_focus_5'\n",
      " 'sensor26__energy_ratio_by_chunks__num_segments_10__segment_focus_6'\n",
      " 'sensor26__energy_ratio_by_chunks__num_segments_10__segment_focus_7'\n",
      " 'sensor26__energy_ratio_by_chunks__num_segments_10__segment_focus_8'\n",
      " 'sensor26__energy_ratio_by_chunks__num_segments_10__segment_focus_9'\n",
      " 'sensor26__fourier_entropy__bins_2' 'sensor26__fourier_entropy__bins_3'\n",
      " 'sensor26__fourier_entropy__bins_5' 'sensor26__fourier_entropy__bins_10'\n",
      " 'sensor26__fourier_entropy__bins_100'\n",
      " 'sensor26__query_similarity_count__query_None__threshold_0.0'\n",
      " 'sensor3__query_similarity_count__query_None__threshold_0.0'\n",
      " 'sensor4__query_similarity_count__query_None__threshold_0.0'\n",
      " 'sensor7__query_similarity_count__query_None__threshold_0.0'\n",
      " 'sensor8__friedrich_coefficients__coeff_0__m_3__r_30'\n",
      " 'sensor8__friedrich_coefficients__coeff_1__m_3__r_30'\n",
      " 'sensor8__friedrich_coefficients__coeff_2__m_3__r_30'\n",
      " 'sensor8__friedrich_coefficients__coeff_3__m_3__r_30'\n",
      " 'sensor8__max_langevin_fixed_point__m_3__r_30'\n",
      " 'sensor8__query_similarity_count__query_None__threshold_0.0'\n",
      " 'sensor9__query_similarity_count__query_None__threshold_0.0'\n",
      " 'sensor11__query_similarity_count__query_None__threshold_0.0'\n",
      " 'sensor12__query_similarity_count__query_None__threshold_0.0'] did not have any finite values. Filling with zeros.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   sensor13__variance_larger_than_standard_deviation  \\\n",
      "1                                                0.0   \n",
      "2                                                0.0   \n",
      "3                                                0.0   \n",
      "4                                                0.0   \n",
      "5                                                0.0   \n",
      "\n",
      "   sensor13__has_duplicate_max  sensor13__has_duplicate_min  \\\n",
      "1                          1.0                          1.0   \n",
      "2                          1.0                          1.0   \n",
      "3                          0.0                          1.0   \n",
      "4                          1.0                          1.0   \n",
      "5                          1.0                          1.0   \n",
      "\n",
      "   sensor13__has_duplicate  sensor13__sum_values  sensor13__abs_energy  \\\n",
      "1                      1.0           1607.338235            573.411548   \n",
      "2                      1.0           1927.529412            531.705882   \n",
      "3                      1.0           1076.838235            269.893382   \n",
      "4                      1.0           1405.323529            421.921713   \n",
      "5                      1.0           1523.073529            361.932742   \n",
      "\n",
      "   sensor13__mean_abs_change  sensor13__mean_change  \\\n",
      "1                   0.002003               0.000090   \n",
      "2                   0.001853               0.000029   \n",
      "3                   0.002203               0.000065   \n",
      "4                   0.001811               0.000028   \n",
      "5                   0.001798               0.000033   \n",
      "\n",
      "   sensor13__mean_second_derivative_central  sensor13__median  ...  \\\n",
      "1                                 -0.000005          0.308824  ...   \n",
      "2                                  0.000007          0.250000  ...   \n",
      "3                                  0.000011          0.235294  ...   \n",
      "4                                  0.000003          0.294118  ...   \n",
      "5                                  0.000003          0.205882  ...   \n",
      "\n",
      "   sensor12__fourier_entropy__bins_5  sensor12__fourier_entropy__bins_10  \\\n",
      "1                           0.235155                            0.305728   \n",
      "2                           0.245901                            0.339942   \n",
      "3                           0.245901                            0.305728   \n",
      "4                           0.249958                            0.316475   \n",
      "5                           0.200814                            0.294982   \n",
      "\n",
      "   sensor12__fourier_entropy__bins_100  \\\n",
      "1                             0.868323   \n",
      "2                             0.871149   \n",
      "3                             0.842774   \n",
      "4                             0.862586   \n",
      "5                             0.824312   \n",
      "\n",
      "   sensor12__permutation_entropy__dimension_3__tau_1  \\\n",
      "1                                           0.189683   \n",
      "2                                           0.180706   \n",
      "3                                           0.200879   \n",
      "4                                           0.198953   \n",
      "5                                           0.181004   \n",
      "\n",
      "   sensor12__permutation_entropy__dimension_4__tau_1  \\\n",
      "1                                           0.285131   \n",
      "2                                           0.271348   \n",
      "3                                           0.300237   \n",
      "4                                           0.298556   \n",
      "5                                           0.271568   \n",
      "\n",
      "   sensor12__permutation_entropy__dimension_5__tau_1  \\\n",
      "1                                           0.378289   \n",
      "2                                           0.360207   \n",
      "3                                           0.398341   \n",
      "4                                           0.396600   \n",
      "5                                           0.360992   \n",
      "\n",
      "   sensor12__permutation_entropy__dimension_6__tau_1  \\\n",
      "1                                           0.472217   \n",
      "2                                           0.449081   \n",
      "3                                           0.496041   \n",
      "4                                           0.495399   \n",
      "5                                           0.450083   \n",
      "\n",
      "   sensor12__permutation_entropy__dimension_7__tau_1  \\\n",
      "1                                           0.567249   \n",
      "2                                           0.537817   \n",
      "3                                           0.594239   \n",
      "4                                           0.591832   \n",
      "5                                           0.539106   \n",
      "\n",
      "   sensor12__query_similarity_count__query_None__threshold_0.0  \\\n",
      "1                                                0.0             \n",
      "2                                                0.0             \n",
      "3                                                0.0             \n",
      "4                                                0.0             \n",
      "5                                                0.0             \n",
      "\n",
      "   sensor12__mean_n_absolute_max__number_of_maxima_7  \n",
      "1                                           0.878160  \n",
      "2                                           0.974414  \n",
      "3                                           0.957356  \n",
      "4                                           0.808102  \n",
      "5                                           0.927505  \n",
      "\n",
      "[5 rows x 14763 columns]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tsfresh import extract_features\n",
    "from tsfresh.utilities.dataframe_functions import roll_time_series, impute\n",
    "from tsfresh.feature_extraction import EfficientFCParameters\n",
    "import logging\n",
    "\n",
    "# Set up logging for debugging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Load the data\n",
    "column_names = ['engine_id', 'cycle', 'setting1', 'setting2', 'setting3'] + [f'sensor{i}' for i in range(1, 27)]\n",
    "train_df = pd.read_csv('./dataset/train_FD001.txt', sep='\\s+', header=None, names=column_names)\n",
    "\n",
    "# Drop columns that aren't informative\n",
    "columns_to_drop = [\"setting3\", \"sensor1\", \"sensor5\", \"sensor6\", \"sensor10\", \"sensor16\", \"sensor18\", \"sensor19\"]\n",
    "train_df_dropped = train_df.drop(columns=columns_to_drop)\n",
    "\n",
    "# Normalize the data\n",
    "scaler = MinMaxScaler()\n",
    "columns_to_skip = train_df_dropped.columns[:2]\n",
    "columns_to_normalize = train_df_dropped.columns[2:]\n",
    "normalized_data = scaler.fit_transform(train_df_dropped[columns_to_normalize])\n",
    "train_df_normalized = pd.concat([train_df_dropped[columns_to_skip].reset_index(drop=True), \n",
    "                                 pd.DataFrame(normalized_data, columns=columns_to_normalize)], axis=1)\n",
    "\n",
    "# Calculate RUL for each cycle\n",
    "train_df_normalized['RUL'] = train_df_normalized.groupby('engine_id')['cycle'].transform(lambda x: x.max() - x)\n",
    "\n",
    "# Sensor columns for later processing\n",
    "sensor_columns = [col for col in train_df_normalized.columns if 'sensor' in col]\n",
    "\n",
    "# Step 1: Apply rolling window on each engine's data\n",
    "# This generates a DataFrame where each row corresponds to a 30-cycle window for each engine\n",
    "df_rolled = roll_time_series(\n",
    "    train_df_normalized,\n",
    "    column_id=\"engine_id\",\n",
    "    column_sort=\"cycle\",\n",
    "    max_timeshift=29,  # Window of 30 cycles\n",
    "    min_timeshift=29\n",
    ")\n",
    "\n",
    "# Step 2: Melt the rolled DataFrame to use with tsfresh\n",
    "melted_df = df_rolled.melt(\n",
    "    id_vars=[\"engine_id\", \"cycle\"],\n",
    "    value_vars=sensor_columns,\n",
    "    var_name=\"kind\",\n",
    "    value_name=\"value\"\n",
    ")\n",
    "\n",
    "# Verify that there are no NaN values\n",
    "if melted_df['value'].isnull().sum() > 0:\n",
    "    melted_df['value'].fillna(0, inplace=True)\n",
    "\n",
    "# Step 3: Feature extraction on each rolling window\n",
    "try:\n",
    "    extracted_features = extract_features(\n",
    "        melted_df,\n",
    "        column_id=\"engine_id\",\n",
    "        column_sort=\"cycle\",\n",
    "        column_kind=\"kind\",\n",
    "        column_value=\"value\",\n",
    "        default_fc_parameters=EfficientFCParameters(),\n",
    "        disable_progressbar=False,\n",
    "        show_warnings=True\n",
    "    )\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error during feature extraction: {e}\")\n",
    "    raise\n",
    "\n",
    "# Step 4: Impute any remaining missing values in extracted features\n",
    "impute(extracted_features)\n",
    "\n",
    "# Display the extracted features\n",
    "print(extracted_features.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 14763)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extracted_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(531930, 25)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_rolled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yshen\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\_array_api.py:695: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmin(X, axis=axis))\n",
      "c:\\Users\\yshen\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\_array_api.py:712: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmax(X, axis=axis))\n",
      "Rolling: 100%|██████████| 28/28 [00:03<00:00,  7.72it/s]\n",
      "C:\\Users\\yshen\\AppData\\Local\\Temp\\ipykernel_22004\\873145974.py:52: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  melted_df['value'].fillna(0, inplace=True)\n",
      "WARNING:tsfresh.feature_extraction.settings:Dependency not available for matrix_profile, this feature will be disabled!\n",
      "Feature Extraction:  20%|██        | 6/30 [22:08<53:51, 134.65s/it]  "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tsfresh import extract_features\n",
    "from tsfresh.utilities.dataframe_functions import roll_time_series, impute\n",
    "from tsfresh.feature_extraction import ComprehensiveFCParameters\n",
    "import logging\n",
    "\n",
    "# Set up logging for debugging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Load the data\n",
    "column_names = ['engine_id', 'cycle', 'setting1', 'setting2', 'setting3'] + [f'sensor{i}' for i in range(1, 27)]\n",
    "train_df = pd.read_csv('./dataset/train_FD001.txt', sep='\\s+', header=None, names=column_names)\n",
    "\n",
    "# Drop columns that aren't informative\n",
    "columns_to_drop = [\"setting3\", \"sensor1\", \"sensor5\", \"sensor6\", \"sensor10\", \"sensor16\", \"sensor18\", \"sensor19\"]\n",
    "train_df_dropped = train_df.drop(columns=columns_to_drop)\n",
    "\n",
    "# Normalize the data\n",
    "scaler = MinMaxScaler()\n",
    "columns_to_skip = train_df_dropped.columns[:2]\n",
    "columns_to_normalize = train_df_dropped.columns[2:]\n",
    "normalized_data = scaler.fit_transform(train_df_dropped[columns_to_normalize])\n",
    "train_df_normalized = pd.concat([train_df_dropped[columns_to_skip].reset_index(drop=True), \n",
    "                                 pd.DataFrame(normalized_data, columns=columns_to_normalize)], axis=1)\n",
    "\n",
    "# Calculate RUL for each cycle\n",
    "train_df_normalized['RUL'] = train_df_normalized.groupby('engine_id')['cycle'].transform(lambda x: x.max() - x)\n",
    "\n",
    "# Sensor columns for later processing\n",
    "sensor_columns = [col for col in train_df_normalized.columns if 'sensor' in col]\n",
    "\n",
    "# Step 1: Apply rolling window on each engine's data\n",
    "df_rolled = roll_time_series(\n",
    "    train_df_normalized,\n",
    "    column_id=\"engine_id\",\n",
    "    column_sort=\"cycle\",\n",
    "    max_timeshift=29,  # Window of 30 cycles\n",
    "    min_timeshift=29\n",
    ")\n",
    "\n",
    "# Step 2: Melt the rolled DataFrame to use with tsfresh\n",
    "melted_df = df_rolled.melt(\n",
    "    id_vars=[\"engine_id\", \"cycle\"],\n",
    "    value_vars=sensor_columns,\n",
    "    var_name=\"kind\",\n",
    "    value_name=\"value\"\n",
    ")\n",
    "\n",
    "# Verify that there are no NaN values\n",
    "melted_df['value'].fillna(0, inplace=True)\n",
    "\n",
    "# Use a comprehensive feature set to ensure all relevant features are captured\n",
    "feature_params = ComprehensiveFCParameters()\n",
    "\n",
    "# Step 3: Extract features on each rolling window\n",
    "try:\n",
    "    extracted_features = extract_features(\n",
    "        melted_df,\n",
    "        column_id=\"engine_id\",\n",
    "        column_sort=\"cycle\",\n",
    "        column_kind=\"kind\",\n",
    "        column_value=\"value\",\n",
    "        default_fc_parameters=feature_params,\n",
    "        disable_progressbar=False,\n",
    "        show_warnings=True\n",
    "    )\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error during feature extraction: {e}\")\n",
    "    raise\n",
    "\n",
    "# Step 4: Impute any remaining missing values in extracted features\n",
    "impute(extracted_features)\n",
    "\n",
    "# Check the feature count\n",
    "print(\"Extracted feature shape:\", extracted_features.shape)\n",
    "print(\"Extracted features head:\\n\", extracted_features.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yshen\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\_array_api.py:695: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmin(X, axis=axis))\n",
      "c:\\Users\\yshen\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\_array_api.py:712: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmax(X, axis=axis))\n",
      "Rolling: 100%|██████████| 28/28 [00:03<00:00,  8.00it/s]\n",
      "C:\\Users\\yshen\\AppData\\Local\\Temp\\ipykernel_24716\\3021206948.py:52: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  melted_df['value'].fillna(0, inplace=True)\n",
      "WARNING:tsfresh.feature_extraction.settings:Dependency not available for matrix_profile, this feature will be disabled!\n",
      "Feature Extraction: 100%|██████████| 20/20 [00:00<00:00, 21.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted feature shape: (100, 190)\n",
      "Extracted features head:\n",
      "    sensor11__sum_values  sensor11__median  sensor11__mean  sensor11__length  \\\n",
      "1            710.654762          0.357143        0.388336            1830.0   \n",
      "2            878.571429          0.267857        0.316033            2780.0   \n",
      "3            572.369048          0.309524        0.336688            1700.0   \n",
      "4            701.297619          0.363095        0.389610            1800.0   \n",
      "5            840.648810          0.285714        0.323326            2600.0   \n",
      "\n",
      "   sensor11__standard_deviation  sensor11__variance  \\\n",
      "1                      0.149070            0.022222   \n",
      "2                      0.165276            0.027316   \n",
      "3                      0.128815            0.016593   \n",
      "4                      0.120972            0.014634   \n",
      "5                      0.160058            0.025619   \n",
      "\n",
      "   sensor11__root_mean_square  sensor11__maximum  sensor11__absolute_maximum  \\\n",
      "1                    0.415965           0.880952                    0.880952   \n",
      "2                    0.356641           0.845238                    0.845238   \n",
      "3                    0.360489           0.910714                    0.910714   \n",
      "4                    0.407958           0.809524                    0.809524   \n",
      "5                    0.360775           0.845238                    0.845238   \n",
      "\n",
      "   sensor11__minimum  ...  sensor9__sum_values  sensor9__median  \\\n",
      "1           0.107143  ...           219.270394         0.120928   \n",
      "2           0.047619  ...           505.058736         0.164094   \n",
      "3           0.053571  ...           438.018307         0.203760   \n",
      "4           0.178571  ...           432.304541         0.179081   \n",
      "5           0.071429  ...           716.338688         0.244234   \n",
      "\n",
      "   sensor9__mean  sensor9__length  sensor9__standard_deviation  \\\n",
      "1       0.119820           1830.0                     0.021277   \n",
      "2       0.181676           2780.0                     0.052806   \n",
      "3       0.257658           1700.0                     0.140342   \n",
      "4       0.240169           1800.0                     0.145843   \n",
      "5       0.275515           2600.0                     0.089130   \n",
      "\n",
      "   sensor9__variance  sensor9__root_mean_square  sensor9__maximum  \\\n",
      "1           0.000453                   0.121694          0.177152   \n",
      "2           0.002789                   0.189195          0.393206   \n",
      "3           0.019696                   0.293400          0.788791   \n",
      "4           0.021270                   0.280983          0.814368   \n",
      "5           0.007944                   0.289573          0.607197   \n",
      "\n",
      "   sensor9__absolute_maximum  sensor9__minimum  \n",
      "1                   0.177152          0.051557  \n",
      "2                   0.393206          0.102710  \n",
      "3                   0.788791          0.113749  \n",
      "4                   0.814368          0.105896  \n",
      "5                   0.607197          0.155658  \n",
      "\n",
      "[5 rows x 190 columns]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tsfresh import extract_features\n",
    "from tsfresh.utilities.dataframe_functions import roll_time_series, impute\n",
    "from tsfresh.feature_extraction import MinimalFCParameters\n",
    "import logging\n",
    "\n",
    "# Set up logging for debugging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Load the data\n",
    "column_names = ['engine_id', 'cycle', 'setting1', 'setting2', 'setting3'] + [f'sensor{i}' for i in range(1, 27)]\n",
    "train_df = pd.read_csv('./dataset/train_FD001.txt', sep='\\s+', header=None, names=column_names)\n",
    "\n",
    "# Drop columns that aren't informative\n",
    "columns_to_drop = [\"setting3\", \"sensor1\", \"sensor5\", \"sensor6\", \"sensor10\", \"sensor16\", \"sensor18\", \"sensor19\"]\n",
    "train_df_dropped = train_df.drop(columns=columns_to_drop)\n",
    "\n",
    "# Normalize the data\n",
    "scaler = MinMaxScaler()\n",
    "columns_to_skip = train_df_dropped.columns[:2]\n",
    "columns_to_normalize = train_df_dropped.columns[2:]\n",
    "normalized_data = scaler.fit_transform(train_df_dropped[columns_to_normalize])\n",
    "train_df_normalized = pd.concat([train_df_dropped[columns_to_skip].reset_index(drop=True), \n",
    "                                 pd.DataFrame(normalized_data, columns=columns_to_normalize)], axis=1)\n",
    "\n",
    "# Calculate RUL for each cycle\n",
    "train_df_normalized['RUL'] = train_df_normalized.groupby('engine_id')['cycle'].transform(lambda x: x.max() - x)\n",
    "\n",
    "# Sensor columns for later processing\n",
    "sensor_columns = [col for col in train_df_normalized.columns if 'sensor' in col]\n",
    "\n",
    "# Step 1: Apply a smaller rolling window to reduce data size\n",
    "df_rolled = roll_time_series(\n",
    "    train_df_normalized,\n",
    "    column_id=\"engine_id\",\n",
    "    column_sort=\"cycle\",\n",
    "    max_timeshift=9,  # Smaller window size of 10 cycles\n",
    "    min_timeshift=9\n",
    ")\n",
    "\n",
    "# Step 2: Melt the rolled DataFrame to use with tsfresh\n",
    "melted_df = df_rolled.melt(\n",
    "    id_vars=[\"engine_id\", \"cycle\"],\n",
    "    value_vars=sensor_columns,\n",
    "    var_name=\"kind\",\n",
    "    value_name=\"value\"\n",
    ")\n",
    "\n",
    "# Verify that there are no NaN values\n",
    "melted_df['value'].fillna(0, inplace=True)\n",
    "\n",
    "# Define a minimal or custom feature set for efficiency\n",
    "feature_params = MinimalFCParameters()\n",
    "\n",
    "# Step 3: Efficient feature extraction on each rolling window\n",
    "try:\n",
    "    extracted_features = extract_features(\n",
    "        melted_df,\n",
    "        column_id=\"engine_id\",\n",
    "        column_sort=\"cycle\",\n",
    "        column_kind=\"kind\",\n",
    "        column_value=\"value\",\n",
    "        default_fc_parameters=feature_params,\n",
    "        disable_progressbar=False,\n",
    "        show_warnings=True,\n",
    "        n_jobs=4  # Parallel processing to speed up extraction\n",
    "    )\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error during feature extraction: {e}\")\n",
    "    raise\n",
    "\n",
    "# Step 4: Impute any remaining missing values in extracted features\n",
    "impute(extracted_features)\n",
    "\n",
    "# Display the extracted features\n",
    "print(\"Extracted feature shape:\", extracted_features.shape)\n",
    "print(\"Extracted features head:\\n\", extracted_features.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 190)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extracted_features.sha\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "pe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yshen\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\_array_api.py:695: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmin(X, axis=axis))\n",
      "c:\\Users\\yshen\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\_array_api.py:712: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmax(X, axis=axis))\n",
      "Rolling: 100%|██████████| 28/28 [00:03<00:00,  7.73it/s]\n",
      "C:\\Users\\yshen\\AppData\\Local\\Temp\\ipykernel_24716\\411599855.py:68: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  melted_df['value'].fillna(0, inplace=True)\n",
      "Feature Extraction: 100%|██████████| 20/20 [00:01<00:00, 17.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted feature shape: (100, 266)\n",
      "Extracted features head:\n",
      "    sensor11__mean  sensor11__standard_deviation  sensor11__minimum  \\\n",
      "1        0.388336                      0.149070           0.107143   \n",
      "2        0.316033                      0.165276           0.047619   \n",
      "3        0.336688                      0.128815           0.053571   \n",
      "4        0.389610                      0.120972           0.178571   \n",
      "5        0.323326                      0.160058           0.071429   \n",
      "\n",
      "   sensor11__maximum  sensor11__variance  sensor11__skewness  \\\n",
      "1           0.880952            0.022222            0.742803   \n",
      "2           0.845238            0.027316            1.071960   \n",
      "3           0.910714            0.016593            1.155786   \n",
      "4           0.809524            0.014634            0.893333   \n",
      "5           0.845238            0.025619            1.007237   \n",
      "\n",
      "   sensor11__kurtosis  sensor11__absolute_sum_of_changes  \\\n",
      "1            0.034417                          13.619048   \n",
      "2            0.511122                          18.434524   \n",
      "3            1.525697                          12.910714   \n",
      "4            0.462804                          12.964286   \n",
      "5            0.621934                          19.958333   \n",
      "\n",
      "   sensor11__longest_strike_below_mean  sensor11__longest_strike_above_mean  \\\n",
      "1                                575.0                                395.0   \n",
      "2                               1315.0                                765.0   \n",
      "3                                315.0                                345.0   \n",
      "4                                180.0                                455.0   \n",
      "5                                675.0                                675.0   \n",
      "\n",
      "   ...  sensor9__variance  sensor9__skewness  sensor9__kurtosis  \\\n",
      "1  ...           0.000453           0.167663           0.085742   \n",
      "2  ...           0.002789           1.164264           0.729071   \n",
      "3  ...           0.019696           1.557639           1.682166   \n",
      "4  ...           0.021270           1.624128           1.880627   \n",
      "5  ...           0.007944           1.349076           1.027247   \n",
      "\n",
      "   sensor9__absolute_sum_of_changes  sensor9__longest_strike_below_mean  \\\n",
      "1                          3.928161                               117.0   \n",
      "2                          5.326842                              1055.0   \n",
      "3                          3.773176                              1115.0   \n",
      "4                          4.157408                              1225.0   \n",
      "5                          6.085704                              1385.0   \n",
      "\n",
      "   sensor9__longest_strike_above_mean  sensor9__count_above_mean  \\\n",
      "1                                70.0                      932.0   \n",
      "2                               655.0                      985.0   \n",
      "3                               485.0                      545.0   \n",
      "4                               515.0                      545.0   \n",
      "5                               655.0                      865.0   \n",
      "\n",
      "   sensor9__count_below_mean  sensor9__linear_trend__attr_\"slope\"  \\\n",
      "1                      898.0                            -0.000017   \n",
      "2                     1795.0                             0.000055   \n",
      "3                     1155.0                             0.000243   \n",
      "4                     1255.0                             0.000235   \n",
      "5                     1735.0                             0.000101   \n",
      "\n",
      "   sensor9__linear_trend__attr_\"intercept\"  \n",
      "1                                 0.135376  \n",
      "2                                 0.105055  \n",
      "3                                 0.051054  \n",
      "4                                 0.028659  \n",
      "5                                 0.143712  \n",
      "\n",
      "[5 rows x 266 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tsfresh import extract_features\n",
    "from tsfresh.utilities.dataframe_functions import roll_time_series, impute\n",
    "import logging\n",
    "\n",
    "# Set up logging for debugging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Load the data\n",
    "column_names = ['engine_id', 'cycle', 'setting1', 'setting2', 'setting3'] + [f'sensor{i}' for i in range(1, 27)]\n",
    "train_df = pd.read_csv('./dataset/train_FD001.txt', sep='\\s+', header=None, names=column_names)\n",
    "\n",
    "# Drop columns that aren't informative\n",
    "columns_to_drop = [\"setting3\", \"sensor1\", \"sensor5\", \"sensor6\", \"sensor10\", \"sensor16\", \"sensor18\", \"sensor19\"]\n",
    "train_df_dropped = train_df.drop(columns=columns_to_drop)\n",
    "\n",
    "# Normalize the data\n",
    "scaler = MinMaxScaler()\n",
    "columns_to_skip = train_df_dropped.columns[:2]\n",
    "columns_to_normalize = train_df_dropped.columns[2:]\n",
    "normalized_data = scaler.fit_transform(train_df_dropped[columns_to_normalize])\n",
    "train_df_normalized = pd.concat([train_df_dropped[columns_to_skip].reset_index(drop=True), \n",
    "                                 pd.DataFrame(normalized_data, columns=columns_to_normalize)], axis=1)\n",
    "\n",
    "# Calculate RUL for each cycle\n",
    "train_df_normalized['RUL'] = train_df_normalized.groupby('engine_id')['cycle'].transform(lambda x: x.max() - x)\n",
    "\n",
    "# Sensor columns for later processing\n",
    "sensor_columns = [col for col in train_df_normalized.columns if 'sensor' in col]\n",
    "\n",
    "# Updated custom feature parameters (replacing 'trend' with 'linear_trend')\n",
    "custom_fc_parameters = {\n",
    "    \"mean\": None,\n",
    "    \"standard_deviation\": None,\n",
    "    \"minimum\": None,\n",
    "    \"maximum\": None,\n",
    "    \"variance\": None,\n",
    "    \"skewness\": None,\n",
    "    \"kurtosis\": None,\n",
    "    \"absolute_sum_of_changes\": None,\n",
    "    \"longest_strike_below_mean\": None,\n",
    "    \"longest_strike_above_mean\": None,\n",
    "    \"count_above_mean\": None,\n",
    "    \"count_below_mean\": None,\n",
    "    \"linear_trend\": [{\"attr\": \"slope\"}, {\"attr\": \"intercept\"}],  # Replaces 'trend'\n",
    "}\n",
    "\n",
    "# Step 1: Apply a rolling window of size 10 cycles for feature extraction\n",
    "df_rolled = roll_time_series(\n",
    "    train_df_normalized,\n",
    "    column_id=\"engine_id\",\n",
    "    column_sort=\"cycle\",\n",
    "    max_timeshift=9,  # Window of 10 cycles\n",
    "    min_timeshift=9\n",
    ")\n",
    "\n",
    "# Step 2: Melt the rolled DataFrame to use with tsfresh\n",
    "melted_df = df_rolled.melt(\n",
    "    id_vars=[\"engine_id\", \"cycle\"],\n",
    "    value_vars=sensor_columns,\n",
    "    var_name=\"kind\",\n",
    "    value_name=\"value\"\n",
    ")\n",
    "\n",
    "# Verify that there are no NaN values\n",
    "melted_df['value'].fillna(0, inplace=True)\n",
    "\n",
    "# Step 3: Feature extraction using custom feature set\n",
    "try:\n",
    "    extracted_features = extract_features(\n",
    "        melted_df,\n",
    "        column_id=\"engine_id\",\n",
    "        column_sort=\"cycle\",\n",
    "        column_kind=\"kind\",\n",
    "        column_value=\"value\",\n",
    "        default_fc_parameters=custom_fc_parameters,  # Using custom feature parameters\n",
    "        disable_progressbar=False,\n",
    "        show_warnings=True,\n",
    "        n_jobs=4  # Use parallel processing for efficiency\n",
    "    )\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error during feature extraction: {e}\")\n",
    "    raise\n",
    "\n",
    "# Step 4: Impute any remaining missing values in extracted features\n",
    "impute(extracted_features)\n",
    "\n",
    "# Display the extracted features\n",
    "print(\"Extracted feature shape:\", extracted_features.shape)\n",
    "print(\"Extracted features head:\\n\", extracted_features.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yshen\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\_array_api.py:695: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmin(X, axis=axis))\n",
      "c:\\Users\\yshen\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\_array_api.py:712: RuntimeWarning: All-NaN slice encountered\n",
      "  return xp.asarray(numpy.nanmax(X, axis=axis))\n",
      "WARNING:tsfresh.feature_extraction.settings:Dependency not available for matrix_profile, this feature will be disabled!\n",
      "Rolling: 100%|██████████| 28/28 [00:03<00:00,  7.60it/s]\n",
      "C:\\Users\\yshen\\AppData\\Local\\Temp\\ipykernel_24716\\1223303113.py:55: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  melted_df['value'].fillna(0, inplace=True)\n",
      "Feature Extraction: 100%|██████████| 20/20 [1:45:21<00:00, 316.06s/it]   \n",
      "c:\\Users\\yshen\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tsfresh\\utilities\\dataframe_functions.py:198: RuntimeWarning: The columns ['sensor11__query_similarity_count__query_None__threshold_0.0'\n",
      " 'sensor12__query_similarity_count__query_None__threshold_0.0'\n",
      " 'sensor13__friedrich_coefficients__coeff_0__m_3__r_30'\n",
      " 'sensor13__friedrich_coefficients__coeff_1__m_3__r_30'\n",
      " 'sensor13__friedrich_coefficients__coeff_2__m_3__r_30'\n",
      " 'sensor13__friedrich_coefficients__coeff_3__m_3__r_30'\n",
      " 'sensor13__max_langevin_fixed_point__m_3__r_30'\n",
      " 'sensor13__query_similarity_count__query_None__threshold_0.0'\n",
      " 'sensor14__query_similarity_count__query_None__threshold_0.0'\n",
      " 'sensor15__query_similarity_count__query_None__threshold_0.0'\n",
      " 'sensor17__friedrich_coefficients__coeff_0__m_3__r_30'\n",
      " 'sensor17__friedrich_coefficients__coeff_1__m_3__r_30'\n",
      " 'sensor17__friedrich_coefficients__coeff_2__m_3__r_30'\n",
      " 'sensor17__friedrich_coefficients__coeff_3__m_3__r_30'\n",
      " 'sensor17__max_langevin_fixed_point__m_3__r_30'\n",
      " 'sensor17__query_similarity_count__query_None__threshold_0.0'\n",
      " 'sensor2__query_similarity_count__query_None__threshold_0.0'\n",
      " 'sensor20__query_similarity_count__query_None__threshold_0.0'\n",
      " 'sensor21__query_similarity_count__query_None__threshold_0.0'\n",
      " 'sensor22__variation_coefficient' 'sensor22__benford_correlation'\n",
      " 'sensor22__autocorrelation__lag_0' 'sensor22__autocorrelation__lag_1'\n",
      " 'sensor22__autocorrelation__lag_2' 'sensor22__autocorrelation__lag_3'\n",
      " 'sensor22__autocorrelation__lag_4' 'sensor22__autocorrelation__lag_5'\n",
      " 'sensor22__autocorrelation__lag_6' 'sensor22__autocorrelation__lag_7'\n",
      " 'sensor22__autocorrelation__lag_8' 'sensor22__autocorrelation__lag_9'\n",
      " 'sensor22__partial_autocorrelation__lag_1'\n",
      " 'sensor22__partial_autocorrelation__lag_2'\n",
      " 'sensor22__partial_autocorrelation__lag_3'\n",
      " 'sensor22__partial_autocorrelation__lag_4'\n",
      " 'sensor22__partial_autocorrelation__lag_5'\n",
      " 'sensor22__partial_autocorrelation__lag_6'\n",
      " 'sensor22__partial_autocorrelation__lag_7'\n",
      " 'sensor22__partial_autocorrelation__lag_8'\n",
      " 'sensor22__partial_autocorrelation__lag_9'\n",
      " 'sensor22__index_mass_quantile__q_0.1'\n",
      " 'sensor22__index_mass_quantile__q_0.2'\n",
      " 'sensor22__index_mass_quantile__q_0.3'\n",
      " 'sensor22__index_mass_quantile__q_0.4'\n",
      " 'sensor22__index_mass_quantile__q_0.6'\n",
      " 'sensor22__index_mass_quantile__q_0.7'\n",
      " 'sensor22__index_mass_quantile__q_0.8'\n",
      " 'sensor22__index_mass_quantile__q_0.9'\n",
      " 'sensor22__fft_aggregated__aggtype_\"centroid\"'\n",
      " 'sensor22__fft_aggregated__aggtype_\"variance\"'\n",
      " 'sensor22__fft_aggregated__aggtype_\"skew\"'\n",
      " 'sensor22__fft_aggregated__aggtype_\"kurtosis\"'\n",
      " 'sensor22__friedrich_coefficients__coeff_0__m_3__r_30'\n",
      " 'sensor22__friedrich_coefficients__coeff_1__m_3__r_30'\n",
      " 'sensor22__friedrich_coefficients__coeff_2__m_3__r_30'\n",
      " 'sensor22__friedrich_coefficients__coeff_3__m_3__r_30'\n",
      " 'sensor22__max_langevin_fixed_point__m_3__r_30'\n",
      " 'sensor22__augmented_dickey_fuller__attr_\"teststat\"__autolag_\"AIC\"'\n",
      " 'sensor22__augmented_dickey_fuller__attr_\"pvalue\"__autolag_\"AIC\"'\n",
      " 'sensor22__augmented_dickey_fuller__attr_\"usedlag\"__autolag_\"AIC\"'\n",
      " 'sensor22__energy_ratio_by_chunks__num_segments_10__segment_focus_0'\n",
      " 'sensor22__energy_ratio_by_chunks__num_segments_10__segment_focus_1'\n",
      " 'sensor22__energy_ratio_by_chunks__num_segments_10__segment_focus_2'\n",
      " 'sensor22__energy_ratio_by_chunks__num_segments_10__segment_focus_3'\n",
      " 'sensor22__energy_ratio_by_chunks__num_segments_10__segment_focus_4'\n",
      " 'sensor22__energy_ratio_by_chunks__num_segments_10__segment_focus_5'\n",
      " 'sensor22__energy_ratio_by_chunks__num_segments_10__segment_focus_6'\n",
      " 'sensor22__energy_ratio_by_chunks__num_segments_10__segment_focus_7'\n",
      " 'sensor22__energy_ratio_by_chunks__num_segments_10__segment_focus_8'\n",
      " 'sensor22__energy_ratio_by_chunks__num_segments_10__segment_focus_9'\n",
      " 'sensor22__fourier_entropy__bins_2' 'sensor22__fourier_entropy__bins_3'\n",
      " 'sensor22__fourier_entropy__bins_5' 'sensor22__fourier_entropy__bins_10'\n",
      " 'sensor22__fourier_entropy__bins_100'\n",
      " 'sensor22__query_similarity_count__query_None__threshold_0.0'\n",
      " 'sensor23__variation_coefficient' 'sensor23__benford_correlation'\n",
      " 'sensor23__autocorrelation__lag_0' 'sensor23__autocorrelation__lag_1'\n",
      " 'sensor23__autocorrelation__lag_2' 'sensor23__autocorrelation__lag_3'\n",
      " 'sensor23__autocorrelation__lag_4' 'sensor23__autocorrelation__lag_5'\n",
      " 'sensor23__autocorrelation__lag_6' 'sensor23__autocorrelation__lag_7'\n",
      " 'sensor23__autocorrelation__lag_8' 'sensor23__autocorrelation__lag_9'\n",
      " 'sensor23__partial_autocorrelation__lag_1'\n",
      " 'sensor23__partial_autocorrelation__lag_2'\n",
      " 'sensor23__partial_autocorrelation__lag_3'\n",
      " 'sensor23__partial_autocorrelation__lag_4'\n",
      " 'sensor23__partial_autocorrelation__lag_5'\n",
      " 'sensor23__partial_autocorrelation__lag_6'\n",
      " 'sensor23__partial_autocorrelation__lag_7'\n",
      " 'sensor23__partial_autocorrelation__lag_8'\n",
      " 'sensor23__partial_autocorrelation__lag_9'\n",
      " 'sensor23__index_mass_quantile__q_0.1'\n",
      " 'sensor23__index_mass_quantile__q_0.2'\n",
      " 'sensor23__index_mass_quantile__q_0.3'\n",
      " 'sensor23__index_mass_quantile__q_0.4'\n",
      " 'sensor23__index_mass_quantile__q_0.6'\n",
      " 'sensor23__index_mass_quantile__q_0.7'\n",
      " 'sensor23__index_mass_quantile__q_0.8'\n",
      " 'sensor23__index_mass_quantile__q_0.9'\n",
      " 'sensor23__fft_aggregated__aggtype_\"centroid\"'\n",
      " 'sensor23__fft_aggregated__aggtype_\"variance\"'\n",
      " 'sensor23__fft_aggregated__aggtype_\"skew\"'\n",
      " 'sensor23__fft_aggregated__aggtype_\"kurtosis\"'\n",
      " 'sensor23__friedrich_coefficients__coeff_0__m_3__r_30'\n",
      " 'sensor23__friedrich_coefficients__coeff_1__m_3__r_30'\n",
      " 'sensor23__friedrich_coefficients__coeff_2__m_3__r_30'\n",
      " 'sensor23__friedrich_coefficients__coeff_3__m_3__r_30'\n",
      " 'sensor23__max_langevin_fixed_point__m_3__r_30'\n",
      " 'sensor23__augmented_dickey_fuller__attr_\"teststat\"__autolag_\"AIC\"'\n",
      " 'sensor23__augmented_dickey_fuller__attr_\"pvalue\"__autolag_\"AIC\"'\n",
      " 'sensor23__augmented_dickey_fuller__attr_\"usedlag\"__autolag_\"AIC\"'\n",
      " 'sensor23__energy_ratio_by_chunks__num_segments_10__segment_focus_0'\n",
      " 'sensor23__energy_ratio_by_chunks__num_segments_10__segment_focus_1'\n",
      " 'sensor23__energy_ratio_by_chunks__num_segments_10__segment_focus_2'\n",
      " 'sensor23__energy_ratio_by_chunks__num_segments_10__segment_focus_3'\n",
      " 'sensor23__energy_ratio_by_chunks__num_segments_10__segment_focus_4'\n",
      " 'sensor23__energy_ratio_by_chunks__num_segments_10__segment_focus_5'\n",
      " 'sensor23__energy_ratio_by_chunks__num_segments_10__segment_focus_6'\n",
      " 'sensor23__energy_ratio_by_chunks__num_segments_10__segment_focus_7'\n",
      " 'sensor23__energy_ratio_by_chunks__num_segments_10__segment_focus_8'\n",
      " 'sensor23__energy_ratio_by_chunks__num_segments_10__segment_focus_9'\n",
      " 'sensor23__fourier_entropy__bins_2' 'sensor23__fourier_entropy__bins_3'\n",
      " 'sensor23__fourier_entropy__bins_5' 'sensor23__fourier_entropy__bins_10'\n",
      " 'sensor23__fourier_entropy__bins_100'\n",
      " 'sensor23__query_similarity_count__query_None__threshold_0.0'\n",
      " 'sensor24__variation_coefficient' 'sensor24__benford_correlation'\n",
      " 'sensor24__autocorrelation__lag_0' 'sensor24__autocorrelation__lag_1'\n",
      " 'sensor24__autocorrelation__lag_2' 'sensor24__autocorrelation__lag_3'\n",
      " 'sensor24__autocorrelation__lag_4' 'sensor24__autocorrelation__lag_5'\n",
      " 'sensor24__autocorrelation__lag_6' 'sensor24__autocorrelation__lag_7'\n",
      " 'sensor24__autocorrelation__lag_8' 'sensor24__autocorrelation__lag_9'\n",
      " 'sensor24__partial_autocorrelation__lag_1'\n",
      " 'sensor24__partial_autocorrelation__lag_2'\n",
      " 'sensor24__partial_autocorrelation__lag_3'\n",
      " 'sensor24__partial_autocorrelation__lag_4'\n",
      " 'sensor24__partial_autocorrelation__lag_5'\n",
      " 'sensor24__partial_autocorrelation__lag_6'\n",
      " 'sensor24__partial_autocorrelation__lag_7'\n",
      " 'sensor24__partial_autocorrelation__lag_8'\n",
      " 'sensor24__partial_autocorrelation__lag_9'\n",
      " 'sensor24__index_mass_quantile__q_0.1'\n",
      " 'sensor24__index_mass_quantile__q_0.2'\n",
      " 'sensor24__index_mass_quantile__q_0.3'\n",
      " 'sensor24__index_mass_quantile__q_0.4'\n",
      " 'sensor24__index_mass_quantile__q_0.6'\n",
      " 'sensor24__index_mass_quantile__q_0.7'\n",
      " 'sensor24__index_mass_quantile__q_0.8'\n",
      " 'sensor24__index_mass_quantile__q_0.9'\n",
      " 'sensor24__fft_aggregated__aggtype_\"centroid\"'\n",
      " 'sensor24__fft_aggregated__aggtype_\"variance\"'\n",
      " 'sensor24__fft_aggregated__aggtype_\"skew\"'\n",
      " 'sensor24__fft_aggregated__aggtype_\"kurtosis\"'\n",
      " 'sensor24__friedrich_coefficients__coeff_0__m_3__r_30'\n",
      " 'sensor24__friedrich_coefficients__coeff_1__m_3__r_30'\n",
      " 'sensor24__friedrich_coefficients__coeff_2__m_3__r_30'\n",
      " 'sensor24__friedrich_coefficients__coeff_3__m_3__r_30'\n",
      " 'sensor24__max_langevin_fixed_point__m_3__r_30'\n",
      " 'sensor24__augmented_dickey_fuller__attr_\"teststat\"__autolag_\"AIC\"'\n",
      " 'sensor24__augmented_dickey_fuller__attr_\"pvalue\"__autolag_\"AIC\"'\n",
      " 'sensor24__augmented_dickey_fuller__attr_\"usedlag\"__autolag_\"AIC\"'\n",
      " 'sensor24__energy_ratio_by_chunks__num_segments_10__segment_focus_0'\n",
      " 'sensor24__energy_ratio_by_chunks__num_segments_10__segment_focus_1'\n",
      " 'sensor24__energy_ratio_by_chunks__num_segments_10__segment_focus_2'\n",
      " 'sensor24__energy_ratio_by_chunks__num_segments_10__segment_focus_3'\n",
      " 'sensor24__energy_ratio_by_chunks__num_segments_10__segment_focus_4'\n",
      " 'sensor24__energy_ratio_by_chunks__num_segments_10__segment_focus_5'\n",
      " 'sensor24__energy_ratio_by_chunks__num_segments_10__segment_focus_6'\n",
      " 'sensor24__energy_ratio_by_chunks__num_segments_10__segment_focus_7'\n",
      " 'sensor24__energy_ratio_by_chunks__num_segments_10__segment_focus_8'\n",
      " 'sensor24__energy_ratio_by_chunks__num_segments_10__segment_focus_9'\n",
      " 'sensor24__fourier_entropy__bins_2' 'sensor24__fourier_entropy__bins_3'\n",
      " 'sensor24__fourier_entropy__bins_5' 'sensor24__fourier_entropy__bins_10'\n",
      " 'sensor24__fourier_entropy__bins_100'\n",
      " 'sensor24__query_similarity_count__query_None__threshold_0.0'\n",
      " 'sensor25__variation_coefficient' 'sensor25__benford_correlation'\n",
      " 'sensor25__autocorrelation__lag_0' 'sensor25__autocorrelation__lag_1'\n",
      " 'sensor25__autocorrelation__lag_2' 'sensor25__autocorrelation__lag_3'\n",
      " 'sensor25__autocorrelation__lag_4' 'sensor25__autocorrelation__lag_5'\n",
      " 'sensor25__autocorrelation__lag_6' 'sensor25__autocorrelation__lag_7'\n",
      " 'sensor25__autocorrelation__lag_8' 'sensor25__autocorrelation__lag_9'\n",
      " 'sensor25__partial_autocorrelation__lag_1'\n",
      " 'sensor25__partial_autocorrelation__lag_2'\n",
      " 'sensor25__partial_autocorrelation__lag_3'\n",
      " 'sensor25__partial_autocorrelation__lag_4'\n",
      " 'sensor25__partial_autocorrelation__lag_5'\n",
      " 'sensor25__partial_autocorrelation__lag_6'\n",
      " 'sensor25__partial_autocorrelation__lag_7'\n",
      " 'sensor25__partial_autocorrelation__lag_8'\n",
      " 'sensor25__partial_autocorrelation__lag_9'\n",
      " 'sensor25__index_mass_quantile__q_0.1'\n",
      " 'sensor25__index_mass_quantile__q_0.2'\n",
      " 'sensor25__index_mass_quantile__q_0.3'\n",
      " 'sensor25__index_mass_quantile__q_0.4'\n",
      " 'sensor25__index_mass_quantile__q_0.6'\n",
      " 'sensor25__index_mass_quantile__q_0.7'\n",
      " 'sensor25__index_mass_quantile__q_0.8'\n",
      " 'sensor25__index_mass_quantile__q_0.9'\n",
      " 'sensor25__fft_aggregated__aggtype_\"centroid\"'\n",
      " 'sensor25__fft_aggregated__aggtype_\"variance\"'\n",
      " 'sensor25__fft_aggregated__aggtype_\"skew\"'\n",
      " 'sensor25__fft_aggregated__aggtype_\"kurtosis\"'\n",
      " 'sensor25__friedrich_coefficients__coeff_0__m_3__r_30'\n",
      " 'sensor25__friedrich_coefficients__coeff_1__m_3__r_30'\n",
      " 'sensor25__friedrich_coefficients__coeff_2__m_3__r_30'\n",
      " 'sensor25__friedrich_coefficients__coeff_3__m_3__r_30'\n",
      " 'sensor25__max_langevin_fixed_point__m_3__r_30'\n",
      " 'sensor25__augmented_dickey_fuller__attr_\"teststat\"__autolag_\"AIC\"'\n",
      " 'sensor25__augmented_dickey_fuller__attr_\"pvalue\"__autolag_\"AIC\"'\n",
      " 'sensor25__augmented_dickey_fuller__attr_\"usedlag\"__autolag_\"AIC\"'\n",
      " 'sensor25__energy_ratio_by_chunks__num_segments_10__segment_focus_0'\n",
      " 'sensor25__energy_ratio_by_chunks__num_segments_10__segment_focus_1'\n",
      " 'sensor25__energy_ratio_by_chunks__num_segments_10__segment_focus_2'\n",
      " 'sensor25__energy_ratio_by_chunks__num_segments_10__segment_focus_3'\n",
      " 'sensor25__energy_ratio_by_chunks__num_segments_10__segment_focus_4'\n",
      " 'sensor25__energy_ratio_by_chunks__num_segments_10__segment_focus_5'\n",
      " 'sensor25__energy_ratio_by_chunks__num_segments_10__segment_focus_6'\n",
      " 'sensor25__energy_ratio_by_chunks__num_segments_10__segment_focus_7'\n",
      " 'sensor25__energy_ratio_by_chunks__num_segments_10__segment_focus_8'\n",
      " 'sensor25__energy_ratio_by_chunks__num_segments_10__segment_focus_9'\n",
      " 'sensor25__fourier_entropy__bins_2' 'sensor25__fourier_entropy__bins_3'\n",
      " 'sensor25__fourier_entropy__bins_5' 'sensor25__fourier_entropy__bins_10'\n",
      " 'sensor25__fourier_entropy__bins_100'\n",
      " 'sensor25__query_similarity_count__query_None__threshold_0.0'\n",
      " 'sensor26__variation_coefficient' 'sensor26__benford_correlation'\n",
      " 'sensor26__autocorrelation__lag_0' 'sensor26__autocorrelation__lag_1'\n",
      " 'sensor26__autocorrelation__lag_2' 'sensor26__autocorrelation__lag_3'\n",
      " 'sensor26__autocorrelation__lag_4' 'sensor26__autocorrelation__lag_5'\n",
      " 'sensor26__autocorrelation__lag_6' 'sensor26__autocorrelation__lag_7'\n",
      " 'sensor26__autocorrelation__lag_8' 'sensor26__autocorrelation__lag_9'\n",
      " 'sensor26__partial_autocorrelation__lag_1'\n",
      " 'sensor26__partial_autocorrelation__lag_2'\n",
      " 'sensor26__partial_autocorrelation__lag_3'\n",
      " 'sensor26__partial_autocorrelation__lag_4'\n",
      " 'sensor26__partial_autocorrelation__lag_5'\n",
      " 'sensor26__partial_autocorrelation__lag_6'\n",
      " 'sensor26__partial_autocorrelation__lag_7'\n",
      " 'sensor26__partial_autocorrelation__lag_8'\n",
      " 'sensor26__partial_autocorrelation__lag_9'\n",
      " 'sensor26__index_mass_quantile__q_0.1'\n",
      " 'sensor26__index_mass_quantile__q_0.2'\n",
      " 'sensor26__index_mass_quantile__q_0.3'\n",
      " 'sensor26__index_mass_quantile__q_0.4'\n",
      " 'sensor26__index_mass_quantile__q_0.6'\n",
      " 'sensor26__index_mass_quantile__q_0.7'\n",
      " 'sensor26__index_mass_quantile__q_0.8'\n",
      " 'sensor26__index_mass_quantile__q_0.9'\n",
      " 'sensor26__fft_aggregated__aggtype_\"centroid\"'\n",
      " 'sensor26__fft_aggregated__aggtype_\"variance\"'\n",
      " 'sensor26__fft_aggregated__aggtype_\"skew\"'\n",
      " 'sensor26__fft_aggregated__aggtype_\"kurtosis\"'\n",
      " 'sensor26__friedrich_coefficients__coeff_0__m_3__r_30'\n",
      " 'sensor26__friedrich_coefficients__coeff_1__m_3__r_30'\n",
      " 'sensor26__friedrich_coefficients__coeff_2__m_3__r_30'\n",
      " 'sensor26__friedrich_coefficients__coeff_3__m_3__r_30'\n",
      " 'sensor26__max_langevin_fixed_point__m_3__r_30'\n",
      " 'sensor26__augmented_dickey_fuller__attr_\"teststat\"__autolag_\"AIC\"'\n",
      " 'sensor26__augmented_dickey_fuller__attr_\"pvalue\"__autolag_\"AIC\"'\n",
      " 'sensor26__augmented_dickey_fuller__attr_\"usedlag\"__autolag_\"AIC\"'\n",
      " 'sensor26__energy_ratio_by_chunks__num_segments_10__segment_focus_0'\n",
      " 'sensor26__energy_ratio_by_chunks__num_segments_10__segment_focus_1'\n",
      " 'sensor26__energy_ratio_by_chunks__num_segments_10__segment_focus_2'\n",
      " 'sensor26__energy_ratio_by_chunks__num_segments_10__segment_focus_3'\n",
      " 'sensor26__energy_ratio_by_chunks__num_segments_10__segment_focus_4'\n",
      " 'sensor26__energy_ratio_by_chunks__num_segments_10__segment_focus_5'\n",
      " 'sensor26__energy_ratio_by_chunks__num_segments_10__segment_focus_6'\n",
      " 'sensor26__energy_ratio_by_chunks__num_segments_10__segment_focus_7'\n",
      " 'sensor26__energy_ratio_by_chunks__num_segments_10__segment_focus_8'\n",
      " 'sensor26__energy_ratio_by_chunks__num_segments_10__segment_focus_9'\n",
      " 'sensor26__fourier_entropy__bins_2' 'sensor26__fourier_entropy__bins_3'\n",
      " 'sensor26__fourier_entropy__bins_5' 'sensor26__fourier_entropy__bins_10'\n",
      " 'sensor26__fourier_entropy__bins_100'\n",
      " 'sensor26__query_similarity_count__query_None__threshold_0.0'\n",
      " 'sensor3__query_similarity_count__query_None__threshold_0.0'\n",
      " 'sensor4__query_similarity_count__query_None__threshold_0.0'\n",
      " 'sensor7__query_similarity_count__query_None__threshold_0.0'\n",
      " 'sensor8__friedrich_coefficients__coeff_0__m_3__r_30'\n",
      " 'sensor8__friedrich_coefficients__coeff_1__m_3__r_30'\n",
      " 'sensor8__friedrich_coefficients__coeff_2__m_3__r_30'\n",
      " 'sensor8__friedrich_coefficients__coeff_3__m_3__r_30'\n",
      " 'sensor8__max_langevin_fixed_point__m_3__r_30'\n",
      " 'sensor8__query_similarity_count__query_None__threshold_0.0'\n",
      " 'sensor9__query_similarity_count__query_None__threshold_0.0'] did not have any finite values. Filling with zeros.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted feature shape: (100, 14877)\n",
      "Extracted features head:\n",
      "    sensor11__variance_larger_than_standard_deviation  \\\n",
      "1                                                0.0   \n",
      "2                                                0.0   \n",
      "3                                                0.0   \n",
      "4                                                0.0   \n",
      "5                                                0.0   \n",
      "\n",
      "   sensor11__has_duplicate_max  sensor11__has_duplicate_min  \\\n",
      "1                          1.0                          1.0   \n",
      "2                          1.0                          0.0   \n",
      "3                          1.0                          1.0   \n",
      "4                          1.0                          1.0   \n",
      "5                          1.0                          1.0   \n",
      "\n",
      "   sensor11__has_duplicate  sensor11__sum_values  sensor11__abs_energy  \\\n",
      "1                      1.0            710.654762            316.639031   \n",
      "2                      1.0            878.571429            353.596088   \n",
      "3                      1.0            572.369048            220.918367   \n",
      "4                      1.0            701.297619            299.573980   \n",
      "5                      1.0            840.648810            338.412309   \n",
      "\n",
      "   sensor11__mean_abs_change  sensor11__mean_change  \\\n",
      "1                   0.007446               0.000254   \n",
      "2                   0.006634               0.000255   \n",
      "3                   0.007599               0.000333   \n",
      "4                   0.007206               0.000285   \n",
      "5                   0.007679               0.000245   \n",
      "\n",
      "   sensor11__mean_second_derivative_central  sensor11__median  ...  \\\n",
      "1                                  0.000013          0.357143  ...   \n",
      "2                                 -0.000035          0.267857  ...   \n",
      "3                                 -0.000088          0.309524  ...   \n",
      "4                                 -0.000010          0.363095  ...   \n",
      "5                                  0.000024          0.285714  ...   \n",
      "\n",
      "   sensor9__fourier_entropy__bins_5  sensor9__fourier_entropy__bins_10  \\\n",
      "1                          0.568273                           0.675651   \n",
      "2                          0.454886                           0.644920   \n",
      "3                          0.045395                           0.155665   \n",
      "4                          0.045395                           0.183378   \n",
      "5                          0.369607                           0.559668   \n",
      "\n",
      "   sensor9__fourier_entropy__bins_100  \\\n",
      "1                            1.878660   \n",
      "2                            1.505063   \n",
      "3                            0.790438   \n",
      "4                            0.797665   \n",
      "5                            1.328059   \n",
      "\n",
      "   sensor9__permutation_entropy__dimension_3__tau_1  \\\n",
      "1                                          0.436226   \n",
      "2                                          0.397719   \n",
      "3                                          0.381002   \n",
      "4                                          0.376830   \n",
      "5                                          0.384542   \n",
      "\n",
      "   sensor9__permutation_entropy__dimension_4__tau_1  \\\n",
      "1                                          0.650738   \n",
      "2                                          0.592030   \n",
      "3                                          0.568034   \n",
      "4                                          0.564092   \n",
      "5                                          0.576564   \n",
      "\n",
      "   sensor9__permutation_entropy__dimension_5__tau_1  \\\n",
      "1                                          0.861492   \n",
      "2                                          0.783440   \n",
      "3                                          0.754064   \n",
      "4                                          0.747357   \n",
      "5                                          0.761470   \n",
      "\n",
      "   sensor9__permutation_entropy__dimension_6__tau_1  \\\n",
      "1                                          1.074358   \n",
      "2                                          0.971751   \n",
      "3                                          0.940664   \n",
      "4                                          0.929580   \n",
      "5                                          0.945354   \n",
      "\n",
      "   sensor9__permutation_entropy__dimension_7__tau_1  \\\n",
      "1                                          1.283193   \n",
      "2                                          1.158882   \n",
      "3                                          1.129209   \n",
      "4                                          1.110684   \n",
      "5                                          1.128180   \n",
      "\n",
      "   sensor9__query_similarity_count__query_None__threshold_0.0  \\\n",
      "1                                                0.0            \n",
      "2                                                0.0            \n",
      "3                                                0.0            \n",
      "4                                                0.0            \n",
      "5                                                0.0            \n",
      "\n",
      "   sensor9__mean_n_absolute_max__number_of_maxima_7  \n",
      "1                                          0.177152  \n",
      "2                                          0.363425  \n",
      "3                                          0.765272  \n",
      "4                                          0.787951  \n",
      "5                                          0.579005  \n",
      "\n",
      "[5 rows x 14877 columns]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tsfresh import extract_features\n",
    "from tsfresh.utilities.dataframe_functions import roll_time_series, impute\n",
    "from tsfresh.feature_extraction import ComprehensiveFCParameters\n",
    "import logging\n",
    "\n",
    "# Set up logging for debugging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Load the data\n",
    "column_names = ['engine_id', 'cycle', 'setting1', 'setting2', 'setting3'] + [f'sensor{i}' for i in range(1, 27)]\n",
    "train_df = pd.read_csv('./dataset/train_FD001.txt', sep='\\s+', header=None, names=column_names)\n",
    "\n",
    "# Drop columns that aren't informative\n",
    "columns_to_drop = [\"setting3\", \"sensor1\", \"sensor5\", \"sensor6\", \"sensor10\", \"sensor16\", \"sensor18\", \"sensor19\"]\n",
    "train_df_dropped = train_df.drop(columns=columns_to_drop)\n",
    "\n",
    "# Normalize the data\n",
    "scaler = MinMaxScaler()\n",
    "columns_to_skip = train_df_dropped.columns[:2]\n",
    "columns_to_normalize = train_df_dropped.columns[2:]\n",
    "normalized_data = scaler.fit_transform(train_df_dropped[columns_to_normalize])\n",
    "train_df_normalized = pd.concat([train_df_dropped[columns_to_skip].reset_index(drop=True), \n",
    "                                 pd.DataFrame(normalized_data, columns=columns_to_normalize)], axis=1)\n",
    "\n",
    "# Calculate RUL for each cycle\n",
    "train_df_normalized['RUL'] = train_df_normalized.groupby('engine_id')['cycle'].transform(lambda x: x.max() - x)\n",
    "\n",
    "# Sensor columns for later processing\n",
    "sensor_columns = [col for col in train_df_normalized.columns if 'sensor' in col]\n",
    "\n",
    "# Use comprehensive feature parameters for a rich set of features\n",
    "comprehensive_fc_parameters = ComprehensiveFCParameters()\n",
    "\n",
    "# Step 1: Apply a rolling window of size 10 cycles for feature extraction\n",
    "df_rolled = roll_time_series(\n",
    "    train_df_normalized,\n",
    "    column_id=\"engine_id\",\n",
    "    column_sort=\"cycle\",\n",
    "    max_timeshift=9,  # Window of 10 cycles\n",
    "    min_timeshift=9\n",
    ")\n",
    "\n",
    "# Step 2: Melt the rolled DataFrame to use with tsfresh\n",
    "melted_df = df_rolled.melt(\n",
    "    id_vars=[\"engine_id\", \"cycle\"],\n",
    "    value_vars=sensor_columns,\n",
    "    var_name=\"kind\",\n",
    "    value_name=\"value\"\n",
    ")\n",
    "\n",
    "# Verify that there are no NaN values\n",
    "melted_df['value'].fillna(0, inplace=True)\n",
    "\n",
    "# Step 3: Feature extraction using comprehensive feature set\n",
    "try:\n",
    "    extracted_features = extract_features(\n",
    "        melted_df,\n",
    "        column_id=\"engine_id\",\n",
    "        column_sort=\"cycle\",\n",
    "        column_kind=\"kind\",\n",
    "        column_value=\"value\",\n",
    "        default_fc_parameters=comprehensive_fc_parameters,  # Comprehensive feature set\n",
    "        disable_progressbar=False,\n",
    "        show_warnings=True,\n",
    "        n_jobs=4  # Use parallel processing for efficiency\n",
    "    )\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error during feature extraction: {e}\")\n",
    "    raise\n",
    "\n",
    "# Step 4: Impute any remaining missing values in extracted features\n",
    "impute(extracted_features)\n",
    "\n",
    "# Display the extracted features\n",
    "print(\"Extracted feature shape:\", extracted_features.shape)\n",
    "print(\"Extracted features head:\\n\", extracted_features.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 14877)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extracted_features.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
