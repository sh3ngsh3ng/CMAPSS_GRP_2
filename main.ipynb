{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define column names\n",
    "columns = [\n",
    "    \"unit_ID\",\n",
    "    \"cycles\",\n",
    "    \"setting_1\",\n",
    "    \"setting_2\",\n",
    "    \"setting_3\",\n",
    "    \"T2\",\n",
    "    \"T24\",\n",
    "    \"T30\",\n",
    "    \"T50\",\n",
    "    \"P2\",\n",
    "    \"P15\",\n",
    "    \"P30\",\n",
    "    \"Nf\",\n",
    "    \"Nc\",\n",
    "    \"epr\",\n",
    "    \"Ps30\",\n",
    "    \"phi\",\n",
    "    \"NRf\",\n",
    "    \"NRc\",\n",
    "    \"BPR\",\n",
    "    \"farB\",\n",
    "    \"htBleed\",\n",
    "    \"Nf_dmd\",\n",
    "    \"PCNfR_dmd\",\n",
    "    \"W31\",\n",
    "    \"W32\",\n",
    "]\n",
    "\n",
    "truth_columns = [\n",
    "    \"unit_ID\",\n",
    "    \"RUL\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#File paths\n",
    "train_path = \"../CMAPSS_GRP_2/dataset/train_FD001.txt\"\n",
    "test_path = \"../CMAPSS_GRP_2/dataset/test_FD001.txt\"\n",
    "rul_path = \"../CMAPSS_GRP_2/dataset/RUL_FD001.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "- Dropping unnecessary columns\n",
    "- Normalization (Min Max Scaling)\n",
    "- Labelling of Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to (a) load data from file paths, (b) drop NaN columns and (c) rename columns.\n",
    "def load_and_clean_data(train_path, test_path, rul_path, columns, truth_columns):\n",
    "    \n",
    "    # Load datasets\n",
    "    train_df = pd.read_csv(train_path, sep=\" \", header=None)\n",
    "    test_df = pd.read_csv(test_path, sep=\" \", header=None)\n",
    "    rul_df = pd.read_csv(rul_path, sep=\" \", header=None)\n",
    "\n",
    "    # Drop NaN columns and rename columns,there exists 2 NaN columns after the 26th data column\n",
    "    train_df.dropna(axis=1, inplace=True)\n",
    "    test_df.dropna(axis=1, inplace=True)\n",
    "    rul_df.dropna(axis=1, inplace=True)\n",
    "    rul_df.insert(0, \"unit_ID\", range(1, len(rul_df) + 1))\n",
    "\n",
    "    train_df.columns = columns\n",
    "    test_df.columns = columns\n",
    "    rul_df.columns = truth_columns\n",
    "\n",
    "    return train_df, test_df, rul_df\n",
    "\n",
    "# 1. Load and clean data\n",
    "train_df, test_df, rul_df = load_and_clean_data(train_path, test_path, rul_path, columns, truth_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Feature Selection: Remove redundant columns based on observations from sensor measurement plots\n",
    "columns_to_remove = [\"T2\", \"P2\", \"P15\", \"epr\", \"farB\", \"Nf_dmd\", \"PCNfR_dmd\"]\n",
    "train_df.drop(columns=columns_to_remove, inplace=True)\n",
    "test_df.drop(columns=columns_to_remove, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New Training Data Generation - Preprocessing\n",
    "- Creating a new training data from existing training data using sliding window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Parameters for sliding windows,shifts,early RUL(piece-wise method) and empty lists to store generated sample data from using shifting window\n",
    "window_length = 30\n",
    "shift = 1\n",
    "early_rul = 125           \n",
    "processed_train_data = []\n",
    "processed_train_targets = []\n",
    "num_test_windows = 5     \n",
    "processed_test_data = []\n",
    "num_test_windows_list = []\n",
    "\n",
    "train_data_first_column = train_df[\"unit_ID\"]\n",
    "test_data_first_column = test_df[\"unit_ID\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Normalisation\n",
    "- Carry out data normalisation on the newly generated training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Perform normalisation with a desired range between 0 and 1.\n",
    "\n",
    "# Initialize the MinMaxScaler with a desired range\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "# Fit and transform the data\n",
    "train_df = scaler.fit_transform(train_df.drop(columns=['unit_ID']))\n",
    "test_df = scaler.transform(test_df.drop(columns=['unit_ID']))\n",
    "\n",
    "train_df = pd.DataFrame(data = np.c_[train_data_first_column, train_df])\n",
    "test_df = pd.DataFrame(data = np.c_[test_data_first_column, test_df])\n",
    "\n",
    "num_train_machines = len(train_df[0].unique())\n",
    "num_test_machines = len(test_df[0].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Normalization\n",
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# # Initialize the scaler\n",
    "# scaler = MinMaxScaler()\n",
    "\n",
    "# # Separate the columns to normalize and the columns to skip\n",
    "# columns_to_skip = train_df_dropped.columns[:2]\n",
    "# columns_to_normalize = train_df_dropped.columns[2:]\n",
    "\n",
    "# # Normalize only the selected columns\n",
    "# normalized_data = scaler.fit_transform(train_df_dropped[columns_to_normalize])\n",
    "\n",
    "# # Combine the normalized and unnormalized columns\n",
    "# train_df_normalized = pd.DataFrame(train_df_dropped[columns_to_skip].values, columns=columns_to_skip)\n",
    "# train_df_normalized = pd.concat([train_df_normalized, pd.DataFrame(normalized_data, columns=columns_to_normalize)], axis=1)\n",
    "\n",
    "# # Display the normalized DataFrame\n",
    "# print(\"Normalized Data (0-1 range):\")\n",
    "# print(train_df_normalized.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Labelling\n",
    "- Labelling of RUL Data using the Piecewise method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to label RUL data using the Piecewise methhod.\n",
    "def process_targets(data_length, early_rul = None):\n",
    "    if early_rul == None:\n",
    "        return np.arange(data_length-1, -1, -1)\n",
    "    else:\n",
    "        early_rul_duration = data_length - early_rul\n",
    "        if early_rul_duration <= 0:\n",
    "            \n",
    "            # Return decreasing RUL based on length of train data if RUL is lesser than specified early RUL (of 125)\n",
    "            return np.arange(data_length-1, -1, -1)\n",
    "        else:\n",
    "            # Return a RUL of constant value (of 125) until it reaches the point when RUL starts to drop below 125.\n",
    "            return np.append(early_rul*np.ones(shape = (early_rul_duration,)), np.arange(early_rul-1, -1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate a new set of training data from existing training data using shifting window method.\n",
    "def process_input_data_with_targets(input_data, target_data = None, window_length = 1, shift = 1):\n",
    "    num_batches = int(np.floor((len(input_data) - window_length)/shift)) + 1\n",
    "    num_features = input_data.shape[1]\n",
    "    output_data = np.repeat(np.nan, repeats = num_batches * window_length * num_features).reshape(num_batches, window_length,\n",
    "                                                                                                  num_features)\n",
    "    if target_data is None:\n",
    "        for batch in range(num_batches):\n",
    "            output_data[batch,:,:] = input_data[(0+shift*batch):(0+shift*batch+window_length),:]\n",
    "        return output_data\n",
    "    else:\n",
    "        output_targets = np.repeat(np.nan, repeats = num_batches)\n",
    "        for batch in range(num_batches):\n",
    "            output_data[batch,:,:] = input_data[(0+shift*batch):(0+shift*batch+window_length),:]\n",
    "            output_targets[batch] = target_data[(shift*batch + (window_length-1))]\n",
    "        return output_data, output_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actual generation a new set of training data from existing training data using shifting window method.\n",
    "for i in np.arange(1, num_train_machines + 1):\n",
    "    \n",
    "    # Temporarily getting train data that belongs to a certain engine number and dropping its engine ID column\n",
    "    temp_train_data = train_df[train_df[0] == i].drop(columns = [0]).values\n",
    "    \n",
    "    # Determine whether it is possible to extract training data with the specified window length.\n",
    "    if (len(temp_train_data) < window_length):\n",
    "        print(\"Train engine {} doesn't have enough data for window_length of {}\".format(i, window_length))\n",
    "        raise AssertionError(\"Window length is larger than number of data points for some engines. \"\n",
    "                             \"Try decreasing window length.\")\n",
    "    \n",
    "    # Generating a RUL target column for every entry in the temporarily train data\n",
    "    temp_train_targets = process_targets(data_length = temp_train_data.shape[0], early_rul = early_rul)\n",
    "    \n",
    "    # Generating a new sample using sliding window for every engine.\n",
    "    data_for_a_machine, targets_for_a_machine = process_input_data_with_targets(temp_train_data, temp_train_targets, \n",
    "                                                                                window_length= window_length, shift = shift)\n",
    "    # Appending the aggregated train data for every engine to a list.\n",
    "    processed_train_data.append(data_for_a_machine)\n",
    "    \n",
    "    # Appending the aggregated RUL target data for every engine to a list.\n",
    "    processed_train_targets.append(targets_for_a_machine)\n",
    "\n",
    "# Compiling the newly generated data (for train and RUL) from sliding window into a dataframe.\n",
    "processed_train_data = np.concatenate(processed_train_data)\n",
    "processed_train_targets = np.concatenate(processed_train_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate test data\n",
    "\n",
    "# Type here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labelling of Data with RUL and Piecewise\n",
    "# # 1) Labelling RUL\n",
    "# train_df_normalized['RUL'] = train_df_normalized.groupby('engine_id')['cycle'].transform(lambda x: x.max() - x)\n",
    "\n",
    "# # 2) Labelling PWRUL\n",
    "# # Set the early RUL threshold\n",
    "# early_rul_threshold = 120\n",
    "\n",
    "# # Define the piecewise linear degradation function\n",
    "# def piecewise_rul(cycle, max_cycle):\n",
    "#     remaining_life = max_cycle - cycle\n",
    "#     if remaining_life > early_rul_threshold:\n",
    "#         return early_rul_threshold  # slower degradation in the early phase\n",
    "#     else:\n",
    "#         return remaining_life  # direct linear degradation after threshold\n",
    "    \n",
    "# train_df_normalized[\"PWRUL\"] = train_df_normalized.apply(lambda row: piecewise_rul(row['cycle'], row['cycle'] + row['RUL']), axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training of Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linera Regresion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Data Preparation for LSTM/Bi-lstm/Cnn-lstm\n",
    "# # Define sequence length\n",
    "# sequence_length = 30\n",
    "\n",
    "# # Identify feature columns\n",
    "# feature_columns = [col for col in train_df_normalized.columns if col not in ['engine_id', 'cycle', 'RUL', 'PWRUL']]\n",
    "\n",
    "# # Initialize lists for sequences and labels\n",
    "# X = []\n",
    "# y = []\n",
    "\n",
    "# # Generate sequences and labels\n",
    "# for engine_id in train_df_normalized['engine_id'].unique():\n",
    "#     engine_data = train_df_normalized[train_df_normalized['engine_id'] == engine_id].reset_index(drop=True)\n",
    "#     for i in range(sequence_length, len(engine_data)):\n",
    "#         # Extract sequence of sensor readings\n",
    "#         seq_x = engine_data[feature_columns].iloc[i-sequence_length:i].values\n",
    "#         # Extract the RUL value at the end of the sequence\n",
    "#         seq_y = engine_data['PWRUL'].iloc[i]\n",
    "#         X.append(seq_x)\n",
    "#         y.append(seq_y)\n",
    "\n",
    "# # Convert to NumPy arrays\n",
    "# X = np.array(X)\n",
    "# y = np.array(y)\n",
    "\n",
    "# print(\"Input shape:\", X.shape)\n",
    "# print(\"Labels shape:\", y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Shuffling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_rul = rul_df[\"RUL\"].values\n",
    "\n",
    "# Shuffle training data\n",
    "index = np.random.permutation(len(processed_train_targets))\n",
    "processed_train_data, processed_train_targets = processed_train_data[index], processed_train_targets[index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform splitting of training data into training and validation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "processed_train_data, processed_val_data, processed_train_targets, processed_val_targets = train_test_split(processed_train_data,\n",
    "                                                                                                            processed_train_targets,\n",
    "                                                                                                            test_size = 0.2,\n",
    "                                                                                                            random_state = 83)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed train data shape:  (14184, 30, 18)\n",
      "Processed validation data shape:  (3547, 30, 18)\n",
      "Processed train targets shape:  (14184,)\n",
      "Processed validation targets shape:  (3547,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Processed train data shape: \", processed_train_data.shape)\n",
    "print(\"Processed validation data shape: \", processed_val_data.shape)\n",
    "print(\"Processed train targets shape: \", processed_train_targets.shape)\n",
    "print(\"Processed validation targets shape: \", processed_val_targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, LSTM, Dense, Bidirectional, Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_compiled_model_LSTM():\n",
    "    model = Sequential([\n",
    "        # CNN layers for spatial feature extraction\n",
    "        layers.Conv1D(64, kernel_size=3, activation=\"relu\", input_shape=(window_length, 18)),\n",
    "        layers.Conv1D(64, kernel_size=3, activation=\"relu\"),\n",
    "        layers.MaxPooling1D(pool_size=2),  # Downsampling to reduce the sequence length\n",
    "        layers.Dropout(0.2),\n",
    "\n",
    "        # LSTM layers for temporal pattern recognition\n",
    "        layers.LSTM(128, return_sequences=True, activation=\"tanh\"),\n",
    "        layers.Dropout(0.2),\n",
    "        layers.LSTM(64, activation=\"tanh\", return_sequences=True),\n",
    "        layers.Dropout(0.2),\n",
    "        layers.LSTM(32, activation=\"tanh\"),\n",
    "        layers.Dropout(0.2),\n",
    "\n",
    "        # Fully connected layers\n",
    "        layers.Dense(96, activation=\"relu\"),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dense(128, activation=\"relu\"),\n",
    "        layers.BatchNormalization(),\n",
    "        \n",
    "        # Output layer\n",
    "        layers.Dense(1)\n",
    "    ])\n",
    "\n",
    "    model.compile(\n",
    "        loss=\"mse\",\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "model_LSTM = create_compiled_model_LSTM()\n",
    "history = model_LSTM.fit(processed_train_data, processed_train_targets, epochs = 5,\n",
    "                    validation_data = (processed_val_data, processed_val_targets),\n",
    "                    batch_size = 128, verbose = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bi-LSTM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_compiled_model_BiLSTM():\n",
    "    model = Sequential([\n",
    "        layers.Bidirectional(layers.LSTM(128, return_sequences=True, activation=\"tanh\"), input_shape=(window_length, 18)),\n",
    "        layers.Bidirectional(layers.LSTM(64, return_sequences=True, activation=\"tanh\")),\n",
    "        layers.Bidirectional(layers.LSTM(32, activation=\"tanh\")),\n",
    "        layers.Dense(96, activation=\"relu\"),\n",
    "        layers.Dense(128, activation=\"relu\"),\n",
    "        layers.Dense(1)\n",
    "    ])\n",
    "    model.compile(loss=\"mse\", optimizer= tf.keras.optimizers.Adam(learning_rate=0.001))\n",
    "    return model\n",
    "\n",
    "model_BiLSTM = create_compiled_model_BiLSTM()\n",
    "history = model_BiLSTM.fit(processed_train_data, processed_train_targets, epochs = 5,\n",
    "                    validation_data = (processed_val_data, processed_val_targets),\n",
    "                    batch_size = 128, verbose = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN-LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Alternative LSTM\n",
    "def create_compiled_model_CNNLSTM():\n",
    "    model = Sequential([\n",
    "        # CNN layers for spatial feature extraction\n",
    "        layers.Conv1D(64, kernel_size=3, activation=\"relu\", input_shape=(window_length, 18)),\n",
    "        layers.Conv1D(64, kernel_size=3, activation=\"relu\"),\n",
    "        layers.MaxPooling1D(pool_size=2),  # Downsampling to reduce the sequence length\n",
    "        layers.Dropout(0.2),\n",
    "\n",
    "        # LSTM layers for temporal pattern recognition\n",
    "        layers.LSTM(128, return_sequences=True, activation=\"tanh\"),\n",
    "        layers.Dropout(0.2),\n",
    "        layers.LSTM(64, activation=\"tanh\", return_sequences=True),\n",
    "        layers.Dropout(0.2),\n",
    "        layers.LSTM(32, activation=\"tanh\"),\n",
    "        layers.Dropout(0.2),\n",
    "\n",
    "        # Fully connected layers\n",
    "        layers.Dense(96, activation=\"relu\"),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dense(128, activation=\"relu\"),\n",
    "        layers.BatchNormalization(),\n",
    "        \n",
    "        # Output layer\n",
    "        layers.Dense(1)\n",
    "    ])\n",
    "\n",
    "    model.compile(\n",
    "        loss=\"mse\",\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "model_CNNLSTM = create_compiled_model_CNNLSTM()\n",
    "history = model_CNNLSTM.fit(processed_train_data, processed_train_targets, epochs = 5,\n",
    "                    validation_data = (processed_val_data, processed_val_targets),\n",
    "                    batch_size = 128, verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the model (CNN-LSTM)\n",
    "cnn_lstm_model = Sequential()\n",
    "cnn_lstm_model.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(sequence_length, len(feature_columns))))\n",
    "cnn_lstm_model.add(Conv1D(filters=64, kernel_size=3, activation='relu'))\n",
    "cnn_lstm_model.add(MaxPooling1D(pool_size=2))\n",
    "cnn_lstm_model.add(LSTM(units=100))\n",
    "cnn_lstm_model.add(Dense(units=1))\n",
    "\n",
    "# Compile the model\n",
    "cnn_lstm_model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "print(cnn_lstm_model.summary())\n",
    "\n",
    "# Training the Model\n",
    "history = cnn_lstm_model.fit(X, y, \n",
    "                    epochs=50, \n",
    "                    batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Models on Test Data\n",
    "- LSTM\n",
    "- Bi-LSTM\n",
    "- CNN+LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepping test data\n",
    "X_test = []\n",
    "y_test = []\n",
    "\n",
    "for engine_id in test_df_normalized['engine_id'].unique():\n",
    "    engine_data = test_df_normalized[test_df_normalized['engine_id'] == engine_id].reset_index(drop=True)\n",
    "    if len(engine_data) >= sequence_length:\n",
    "        # Use only the last sequence\n",
    "        seq_x = engine_data[feature_columns].iloc[-sequence_length:].values\n",
    "        X_test.append(seq_x)\n",
    "        # Get the true RUL for this engine\n",
    "        seq_y = true_rul.loc[engine_id - 1].values[0]\n",
    "        y_test.append(seq_y)\n",
    "    else:\n",
    "        print(f\"Engine {engine_id} has insufficient data for the defined sequence length.\")\n",
    "\n",
    "X_test = np.array(X_test)\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regresssion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bi-LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN-LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Predict RUL on test data\n",
    "y_test_pred = cnn_lstm_model.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "test_mse = mean_squared_error(y_test, y_test_pred)\n",
    "print(\"Test Mean Squared Error:\", test_mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing the Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
